{
 "cells": [
  {
   "cell_type": "raw",
   "id": "df3fbcdd",
   "metadata": {},
   "source": [
    "conda amazonei tensorflow_p36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4663e665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success - the MySageMakerInstance is in the us-west-2 region. You will use the 433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest container for your SageMaker endpoint.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import boto3, re, sys, math, json, os, sagemaker, urllib.request\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "prefix = 'sagemaker/DEMO-xgboost-dm'\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", my_region, \"latest\")\n",
    "\n",
    "print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + xgboost_container + \" container for your SageMaker endpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf260ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62f65b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 error:  An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'awsgis' # <--- CHANGE THIS VARIABLE TO A UNIQUE NAME FOR YOUR BUCKET\n",
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "      s3.create_bucket(Bucket=bucket_name)\n",
    "    else: \n",
    "      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5df9b35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: uszipcode in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (0.2.6)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from uszipcode) (21.2.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from uszipcode) (2.26.0)\n",
      "Requirement already satisfied: pathlib-mate in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from uszipcode) (1.0.1)\n",
      "Requirement already satisfied: SQLAlchemy in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from uszipcode) (1.3.13)\n",
      "Requirement already satisfied: autopep8 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from pathlib-mate->uszipcode) (1.4.4)\n",
      "Requirement already satisfied: atomicwrites in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from pathlib-mate->uszipcode) (1.3.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from pathlib-mate->uszipcode) (1.16.0)\n",
      "Requirement already satisfied: pycodestyle>=2.4.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from autopep8->pathlib-mate->uszipcode) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from requests->uszipcode) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from requests->uszipcode) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from requests->uszipcode) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages (from requests->uszipcode) (1.26.7)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install uszipcode\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout\n",
    "from tensorflow.keras.models import model_from_json, load_model\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "NUM_SAMPLES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "797fcc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(_input_dir, _output_dir=\"output\", _data_size=-1):\n",
    "    \"\"\"\n",
    "    :param _input_dir: input directory name\n",
    "                      AWS S3 directory name, where the input files are stored\n",
    "    :param _output_dir: output directory name\n",
    "                      AWS S3 directory name, where the output files are saved\n",
    "    :param _data_size: size of data\n",
    "                      Data size, that needs to be tested, by default it takes value of\n",
    "                      -1, which means consider all the data\n",
    "    :return:\n",
    "            the processed data, and demand data\n",
    "    \"\"\"\n",
    "    import os.path\n",
    "    import pandas as pd\n",
    "    from pandas import DataFrame\n",
    "    from uszipcode import SearchEngine\n",
    "\n",
    "    # to obtain the zip-codes for latitude, longitude values\n",
    "    engine = SearchEngine()\n",
    "\n",
    "    # load all the data\n",
    "    months = [\"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\"]\n",
    "    file_format = \"uber-raw-data-{}14.csv\"\n",
    "    _data = DataFrame()\n",
    "    for month in months:\n",
    "        file_name = _input_dir + \"/\" + file_format.format(month)\n",
    "        df_sub = pd.read_csv(file_name)\n",
    "        _data = _data.append(df_sub)\n",
    "\n",
    "    # sample the data\n",
    "    if _data_size > 0:\n",
    "        _data = _data.sample(n=_data_size)\n",
    "\n",
    "    # process date and time\n",
    "    _data['Date/Time'] = pd.to_datetime(_data['Date/Time'], format='%m/%d/%Y %H:%M:%S')\n",
    "    _data['month'] = _data['Date/Time'].dt.month\n",
    "    _data['weekday'] = _data['Date/Time'].dt.dayofweek\n",
    "    _data['day'] = _data['Date/Time'].dt.day\n",
    "    _data['hour'] = _data['Date/Time'].dt.hour\n",
    "    _data['minute'] = _data['Date/Time'].dt.minute\n",
    "    _data['lat_short'] = round(_data['Lat'], 2)\n",
    "    _data['lon_short'] = round(_data['Lon'], 2)\n",
    "\n",
    "    # obtaining the zip-codes\n",
    "    _data['zip'] = _data.apply(\n",
    "        lambda row: engine.by_coordinates(row['Lat'], row['Lon'], radius=10)[0].zipcode, axis=1\n",
    "    )\n",
    "\n",
    "    # summarizing demand data\n",
    "    _demand = (_data.groupby(['zip']).count()['Date/Time']).reset_index()\n",
    "    _demand.columns = ['Zip', 'Number of Trips']\n",
    "    _demand.to_csv(_output_dir + \"/demand.csv\", index=False)\n",
    "\n",
    "    _demand_w = (_data.groupby(['zip', 'weekday']).count()['Date/Time']).reset_index()\n",
    "    _demand_w.columns = ['Zip', 'Weekday', 'Number of Trips']\n",
    "    _demand_w.to_csv(_output_dir + \"/demand_dow.csv\", index=False)\n",
    "\n",
    "    _demand_h = (_data.groupby(['zip', 'hour']).count()['Date/Time']).reset_index()\n",
    "    _demand_h.columns = ['Zip', 'Hour', 'Number of Trips']\n",
    "    _demand_h.to_csv(_output_dir + \"/demand_h.csv\", index=False)\n",
    "\n",
    "    _demand_wh = (_data.groupby(['zip', 'weekday', 'hour']).count()['Date/Time']).reset_index()\n",
    "    _demand_wh.columns = ['Zip', 'Weekday', 'Hour', 'Number of Trips']\n",
    "    _demand_wh.to_csv(_output_dir + \"/demand_h_dow.csv\", index=False)\n",
    "\n",
    "    return _data, _demand, _demand_w, _demand_h, _demand_wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abef7613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemandPredictorBase(object):\n",
    "    \"\"\"\n",
    "        Base class for demand predictor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, _x, _y, _prefix, train=True, output_dir=\".\"):\n",
    "        self.x = _x\n",
    "        self.y = _y\n",
    "        self.prefix = _prefix\n",
    "        self.output_dir = output_dir\n",
    "        self.model = self.build_model()\n",
    "        if train:\n",
    "            self.train()\n",
    "\n",
    "    def build_model(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, _x_test):\n",
    "        \"\"\"\n",
    "        :param _x_test: test dataset\n",
    "        :return: prediction for the given test dataset _x_test\n",
    "        \"\"\"\n",
    "        return self.model.predict(_x_test)\n",
    "\n",
    "    def predict_and_scale(self, _x_test, _y_scalar):\n",
    "        \"\"\"\n",
    "        :param _x_test: test dataset\n",
    "        :param _y_scalar: Scaler\n",
    "        :return: prediction for the given test dataset _x_test, scaled to the scalar\n",
    "        \"\"\"\n",
    "        return _y_scalar.inverse_transform(self.predict(_x_test))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mse(_y_test, _y_pred):\n",
    "        \"\"\"\n",
    "        :param _y_test: actual test values\n",
    "        :param _y_pred: predicted test values\n",
    "        :return: return the mean square error\n",
    "        \"\"\"\n",
    "        return mean_squared_error(_y_test, _y_pred)\n",
    "    \n",
    "    def save_model(self, _data_size, _model_id):\n",
    "        if self.model is not None:\n",
    "            filename = f'{self.output_dir}/{self.prefix}_model_{_data_size}_{_model_id}.pickle'\n",
    "            pickle.dump(self.model, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "class DemandPredictorNN(DemandPredictorBase):\n",
    "    def __init__(self, _x, _y, train=True, output_dir=\".\", hidden_layer_conf=None, d_key=\"\", dt_size=-1):\n",
    "        self.input_shape = len(_x[0])\n",
    "        self.output_shape = len(_y[0])\n",
    "        self.hidden_layer_conf = hidden_layer_conf\n",
    "        self.d_key = d_key\n",
    "        self.dt_size = dt_size\n",
    "        self.epochs = 4000\n",
    "        self.batch_size = 150\n",
    "        self.verbose = 0\n",
    "        self.validation_split = 0.2\n",
    "        self.learning_rate = 0.01\n",
    "        self.history = None\n",
    "        super(DemandPredictorNN, self).__init__(_x, _y, \"nn\", train, output_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            build the model\n",
    "        \"\"\"\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=(self.input_shape,)))\n",
    "        if self.hidden_layer_conf is None:\n",
    "            model.add(Dense(168, activation='relu'))\n",
    "            model.add(Dropout(0.1))\n",
    "            model.add(Dense(24, activation='relu'))\n",
    "            model.add(Dropout(0.01))\n",
    "            model.add(Dense(self.output_shape, activation='linear'))\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        else:\n",
    "            for layer in self.hidden_layer_conf:\n",
    "                model.add(Dense(layer[\"neurons\"], activation=layer[\"activation\"]))\n",
    "        model.add(Dense(self.output_shape, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def predict(self, _x_test):\n",
    "        if os.path.exists(self.output_dir + f\"/best_model_{self.d_key}_{self.dt_size}.h5\"):\n",
    "            self.model = load_model(self.output_dir + f\"/best_model_{self.d_key}_{self.dt_size}.h5\")\n",
    "        return super(DemandPredictorNN, self).predict(_x_test)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            train the model\n",
    "        \"\"\"\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=self.verbose, patience=100)\n",
    "        mc = ModelCheckpoint(\n",
    "            self.output_dir + f'/best_model_{self.d_key}_{self.dt_size}.h5', monitor='val_loss', mode='min',\n",
    "            verbose=self.verbose, save_best_only=True\n",
    "        )\n",
    "        self.history = \\\n",
    "            self.model.fit(\n",
    "                self.x, self.y,\n",
    "                epochs=self.epochs, batch_size=self.batch_size,\n",
    "                verbose=self.verbose, validation_split=self.validation_split,\n",
    "                use_multiprocessing=True, callbacks=[es, mc]\n",
    "            )\n",
    "\n",
    "    def plot(self, _i, _mse=None):\n",
    "        losses = self.history.history['loss']\n",
    "        val_losses = self.history.history['val_loss']\n",
    "        epochs = [i for i in range(len(losses))]\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel(\"Epochs [Log Scale]\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        legend = [\"Val Loss\", \"Train Loss\"]\n",
    "        if _mse is not None:\n",
    "            plt.plot(epochs, [_mse for _ in range(len(epochs))])\n",
    "            legend = [\"MSE\"] + legend\n",
    "        plt.plot(epochs, val_losses)\n",
    "        plt.plot(epochs, losses)\n",
    "        plt.legend(legend, loc=\"lower center\", bbox_to_anchor=(0.5, 0.0))\n",
    "        plt.title(\"Variation of Loss function over time\")\n",
    "        plt.show()\n",
    "#         plt.savefig(f'{_i}.png')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "class DemandPredictorSVR(DemandPredictorBase):\n",
    "    def __init__(self, _x, _y, train=True, output_dir=\".\"):\n",
    "        self.kernel = 'rbf'\n",
    "        self.gamma = 10\n",
    "        self.c = 10\n",
    "        super(DemandPredictorSVR, self).__init__(_x, _y, \"svr\", train, output_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            build the model\n",
    "        \"\"\"\n",
    "        model = SVR(kernel=self.kernel, gamma=self.gamma, C=self.c)\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            train the model\n",
    "        \"\"\"\n",
    "        self.model.fit(self.x, self.y)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "class DemandPredictorNB(DemandPredictorBase):\n",
    "    def __init__(self, _x, _y, train=True, output_dir=\".\"):\n",
    "\n",
    "        super(DemandPredictorNB, self).__init__(_x, _y, \"nb\", train, output_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            build the model\n",
    "        \"\"\"\n",
    "        model = GaussianNB()\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            train the model\n",
    "        \"\"\"\n",
    "        self.model.fit(self.x, self.y)\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "class DemandPredictorTree(DemandPredictorBase):\n",
    "    def __init__(self, _x, _y, train=True, output_dir=\".\"):\n",
    "        self.criterion = 'entropy'\n",
    "        self.max_depth = 10\n",
    "        self.splitter = 'best'\n",
    "        super(DemandPredictorTree, self).__init__(_x, _y, \"dt\", train, output_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            build the model\n",
    "        \"\"\"\n",
    "        model = tree.DecisionTreeClassifier(criterion = self.criterion, splitter = self.splitter, max_depth = self.max_depth)\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            train the model\n",
    "        \"\"\"\n",
    "        self.model.fit(self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9d79cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(_processed_data):\n",
    "    columns = list(_processed_data.columns)\n",
    "    columns.remove(\"Number of Trips\")\n",
    "    sc_x = StandardScaler()\n",
    "    sc_y = MinMaxScaler()\n",
    "    x = np.array(\n",
    "        [\n",
    "            [entry[col] for col in columns]\n",
    "            for _, entry in _processed_data.iterrows()\n",
    "        ]\n",
    "    )\n",
    "    y = np.transpose([_processed_data[\"Number of Trips\"].to_list()])\n",
    "    x = sc_x.fit_transform(x)\n",
    "    y = sc_y.fit_transform(y)\n",
    "    return x, y, sc_x, sc_y\n",
    "\n",
    "def solve_using_neural_network(_processed_data, _demand_key, _output_dir, _hidden_layer_config, _data_size):\n",
    "    \"\"\"\n",
    "    :param _processed_data: processed_data\n",
    "    :param _demand_key: demand data type\n",
    "    :param _output_dir: output directory\n",
    "    :param _hidden_layer_config: hidden layer configuration\n",
    "    :param _data_size: data size\n",
    "    :return: run NUM_SAMPLES time neural network and compute the average MSE and MSE ratio\n",
    "    \"\"\"\n",
    "    _x, _y, _, _ = transform_data(_processed_data)\n",
    "\n",
    "    mse = []\n",
    "    time_taken = []\n",
    "\n",
    "    for _i in range(NUM_SAMPLES):\n",
    "        start = datetime.now()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.1, random_state=_i)\n",
    "        tf.random.set_seed(_i)\n",
    "        nn = DemandPredictorNN(\n",
    "            _x=x_train, _y=y_train,\n",
    "            output_dir=_output_dir, hidden_layer_conf=_hidden_layer_config,\n",
    "            d_key=_demand_key, dt_size=_data_size\n",
    "        )\n",
    "        y_pred = nn.predict(x_test)\n",
    "        mse_val = nn.get_mse(y_test, y_pred)\n",
    "        nn.plot(f\"{_output_dir}/demand_type_{_demand_key}_sample_id_{_i}\", mse_val)\n",
    "        mse.append(mse_val)\n",
    "        time_taken.append((datetime.now() - start).total_seconds())\n",
    "\n",
    "    print(f\"Average Time Taken: {np.mean(time_taken)} seconds\")\n",
    "    print(f\"Average MSE: {np.mean(mse)}, Min MSE: {min(mse)}, Max MSE: {max(mse)}\")\n",
    "\n",
    "def solve_using_svr(_processed_data, _demand_key, _output_dir, _data_size):\n",
    "    \"\"\"\n",
    "    :param _processed_data: processed_data\n",
    "    :param _demand_key: demand data type\n",
    "    :param _output_dir: output directory\n",
    "    :param _data_size: data size\n",
    "    :return: run NUM_SAMPLES time neural network and compute the average MSE and MSE ratio\n",
    "    \"\"\"\n",
    "    _x, _y, _, _ = transform_data(_processed_data)\n",
    "\n",
    "    mse = []\n",
    "    time_taken = []\n",
    "\n",
    "    for _i in range(NUM_SAMPLES):\n",
    "        start = datetime.now()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.1, random_state=_i)\n",
    "        svr = DemandPredictorSVR(x_train, y_train, output_dir=_output_dir)\n",
    "        y_pred = svr.predict(x_test)\n",
    "        mse.append(svr.get_mse(y_test, y_pred))\n",
    "        time_taken.append((datetime.now() - start).total_seconds())\n",
    "        svr.save_model(_data_size, _i)\n",
    "\n",
    "    print(f\"Average Time Taken: {np.mean(time_taken)} seconds\")\n",
    "    print(f\"Average MSE: {np.mean(mse)}, Min MSE: {min(mse)}, Max MSE: {max(mse)}\")\n",
    "\n",
    "def solve_using_nb(_processed_data, _demand_key, _output_dir, _data_size):\n",
    "    \"\"\"\n",
    "    :param _processed_data: processed_data\n",
    "    :param _demand_key: demand data type\n",
    "    :param _output_dir: output directory\n",
    "    :param _data_size: data size\n",
    "    :return: run NUM_SAMPLES time neural network and compute the average MSE and MSE ratio\n",
    "    \"\"\"\n",
    "    _x, _y, _, _ = transform_data(_processed_data)\n",
    "\n",
    "    mse = []\n",
    "    time_taken = []\n",
    "\n",
    "    for _i in range(NUM_SAMPLES):\n",
    "        start = datetime.now()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.1, random_state=_i)\n",
    "        nb = DemandPredictorNB(x_train, y_train, output_dir=_output_dir)\n",
    "        y_pred = nb.predict(x_test)\n",
    "        mse.append(nb.get_mse(y_test, y_pred))\n",
    "        time_taken.append((datetime.now() - start).total_seconds())\n",
    "        nb.save_model(_data_size, _i)\n",
    "\n",
    "    print(f\"Average Time Taken: {np.mean(time_taken)} seconds\")\n",
    "    print(f\"Average MSE: {np.mean(mse)}, Min MSE: {min(mse)}, Max MSE: {max(mse)}\")\n",
    "\n",
    "def solve_using_tree(_processed_data, _demand_key, _output_dir, _data_size):\n",
    "    \"\"\"\n",
    "    :param _processed_data: processed_data\n",
    "    :param _demand_key: demand data type\n",
    "    :param _output_dir: output directory\n",
    "    :param _data_size: data size\n",
    "    :return: run NUM_SAMPLES time neural network and compute the average MSE and MSE ratio\n",
    "    \"\"\"\n",
    "    _x, _y, _, _ = transform_data(_processed_data)\n",
    "\n",
    "    mse = []\n",
    "    time_taken = []\n",
    "\n",
    "    for _i in range(NUM_SAMPLES):\n",
    "        start = datetime.now()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.1, random_state=_i)\n",
    "        clf = DemandPredictorTree(x_train, y_train, output_dir=_output_dir)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        mse.append(clf.get_mse(y_test, y_pred))\n",
    "        time_taken.append((datetime.now() - start).total_seconds())\n",
    "        clf.save_model(_data_size, _i)\n",
    "\n",
    "    print(f\"Average Time Taken: {np.mean(time_taken)} seconds\")\n",
    "    print(f\"Average MSE: {np.mean(mse)}, Min MSE: {min(mse)}, Max MSE: {max(mse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73238d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function(_data_size, _input_dir, _output_dir):\n",
    "    \"\"\"\n",
    "    :param _data_size: size of the input data\n",
    "    :param _input_dir: input directory\n",
    "    :param _output_dir: output directory\n",
    "    \"\"\"\n",
    "    data, _, _, _, demand_wh = load_data(\n",
    "        _input_dir=_input_dir, _data_size=_data_size, _output_dir=_output_dir\n",
    "    )\n",
    "    nn_hidden_layer_config = {\n",
    "        \"weekday_n_hour\": [\n",
    "            {\n",
    "                \"activation\": \"relu\", \"neurons\": 168\n",
    "            },\n",
    "            {\n",
    "                \"activation\": \"relu\", \"neurons\": 24\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    demand_data = {\n",
    "        \"weekday_n_hour\": demand_wh\n",
    "    }\n",
    "    for demand_key in demand_data:\n",
    "        demand_datum = demand_data[demand_key]\n",
    "        print(f\"checking {demand_key}\")\n",
    "        solve_using_svr(demand_datum, demand_key, _output_dir=_output_dir, _data_size=_data_size)\n",
    "        solve_using_nb(demand_datum, demand_key, _output_dir=_output_dir, _data_size=_data_size)\n",
    "        solve_using_tree(demand_datum, demand_key, _output_dir=_output_dir, _data_size=_data_size)\n",
    "        solve_using_neural_network(\n",
    "            demand_datum, demand_key, _output_dir=_output_dir,\n",
    "            _hidden_layer_config=nn_hidden_layer_config[demand_key], _data_size=_data_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b1f6e8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/s3fs/core.py\u001b[0m in \u001b[0;36m_call_s3\u001b[0;34m(self, method, *akwarglist, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mawait\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0madditional_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/aiobotocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (403) when calling the HeadObject operation: Forbidden",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2bf8dc6344e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"s3://cloud-project-x\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"s3://cloud-project-x/output_{data_size}_{notebook_instance_id}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-ba4a27bdfd62>\u001b[0m in \u001b[0;36mmain_function\u001b[0;34m(_data_size, _input_dir, _output_dir)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m     data, _, _, _, demand_wh = load_data(\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0m_input_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_input_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_data_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_data_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_output_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_output_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     nn_hidden_layer_config = {\n",
      "\u001b[0;32m<ipython-input-13-df4508f608ad>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(_input_dir, _output_dir, _data_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmonth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmonths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_input_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mdf_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_sub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;31m# See https://github.com/python/mypy/issues/1297\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m     )\n\u001b[1;32m    439\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0mstorage_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"anon\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             file_obj = fsspec.open(\n\u001b[0;32m--> 233\u001b[0;31m                 \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m             ).open()\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/fsspec/core.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mbeen\u001b[0m \u001b[0mdeleted\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mbut\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32mwith\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mbetter\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mcloser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mfobjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/fsspec/core.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfobjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, block_size, cache_options, **kwargs)\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0mautocommit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mac\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m                 \u001b[0mcache_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m             )\n\u001b[1;32m    950\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mac\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/s3fs/core.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, path, mode, block_size, acl, version_id, fill_cache, cache_type, autocommit, requester_pays, **kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mcache_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0mautocommit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautocommit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             \u001b[0mrequester_pays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequester_pays\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         )\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/s3fs/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, s3, path, mode, block_size, acl, version_id, fill_cache, s3_additional_kwargs, autocommit, cache_type, requester_pays)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"VersionId\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m         super().__init__(\n\u001b[0;32m-> 1661\u001b[0;31m             \u001b[0ms3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautocommit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautocommit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1662\u001b[0m         )\n\u001b[1;32m   1663\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m  \u001b[0;31m# compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/fsspec/spec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fs, path, mode, block_size, autocommit, cache_type, cache_options, **kwargs)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"details\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetails\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m             self.cache = caches[cache_type](\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/fsspec/asyn.py\u001b[0m in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mcoro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoro\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mawait\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/s3fs/core.py\u001b[0m in \u001b[0;36m_info\u001b[0;34m(self, path, bucket, key, refresh, version_id)\u001b[0m\n\u001b[1;32m    915\u001b[0m                     \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mversion_id_kw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m                     \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreq_kw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m                 )\n\u001b[1;32m    919\u001b[0m                 return {\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/s3fs/core.py\u001b[0m in \u001b[0;36m_call_s3\u001b[0;34m(self, method, *akwarglist, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mtranslate_boto_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0mcall_s3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msync_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_call_s3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: Forbidden"
     ]
    }
   ],
   "source": [
    "data_size = 1000 # 100\n",
    "notebook_instance_id = 5 #1\n",
    "input_dir = \"s3://cloud-project-x\"\n",
    "output_dir = f\"s3://cloud-project-x/output_{data_size}_{notebook_instance_id}\"\n",
    "main_function(data_size, input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4116c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6115a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef836f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
