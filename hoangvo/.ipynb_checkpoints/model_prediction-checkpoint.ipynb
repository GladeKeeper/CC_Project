{
 "cells": [
  {
   "cell_type": "raw",
   "id": "87acd61a",
   "metadata": {},
   "source": [
    "conda amazonei tensorflow_p36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3a750ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sagemaker'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-93bafa7515da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msagemaker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_execution_role\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sagemaker'"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "prefix = 'sagemaker/DEMO-xgboost-dm'\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", my_region, \"latest\")\n",
    "\n",
    "print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + xgboost_container + \" container for your SageMaker endpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30a22c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 error:  name 'my_region' is not defined\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'awsgis' # <--- CHANGE THIS VARIABLE TO A UNIQUE NAME FOR YOUR BUCKET\n",
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "      s3.create_bucket(Bucket=bucket_name)\n",
    "    else: \n",
    "      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b31ac03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "NUM_SAMPLES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4adb0d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(_input_dir, _output_dir=\"output\", _data_size=-1):\n",
    "    \"\"\"\n",
    "    :param _input_dir: input directory name\n",
    "                      AWS S3 directory name, where the input files are stored\n",
    "    :param _output_dir: output directory name\n",
    "                      AWS S3 directory name, where the output files are saved\n",
    "    :param _data_size: size of data\n",
    "                      Data size, that needs to be tested, by default it takes value of\n",
    "                      -1, which means consider all the data\n",
    "    :return:\n",
    "            the processed data, and demand data\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from pandas import DataFrame\n",
    "\n",
    "    # load all the data\n",
    "    months = [\"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\"]\n",
    "    file_format = \"uber-processed-data-{}14.csv\"\n",
    "    _data = DataFrame()\n",
    "    for month in months:\n",
    "        file_name = _input_dir + \"/\" + file_format.format(month)\n",
    "        df_sub = pd.read_csv(file_name)\n",
    "        _data = _data.append(df_sub)\n",
    "\n",
    "    # sample the data\n",
    "    if _data_size > 0:\n",
    "        _data = _data.sample(n=_data_size)\n",
    "\n",
    "    # summarizing demand data\n",
    "    _demand = (_data.groupby(['zip']).count()['Date/Time']).reset_index()\n",
    "    _demand.columns = ['Zip', 'Number of Trips']\n",
    "\n",
    "    _demand_w = (_data.groupby(['zip', 'weekday']).count()['Date/Time']).reset_index()\n",
    "    _demand_w.columns = ['Zip', 'Weekday', 'Number of Trips']\n",
    "\n",
    "    _demand_h = (_data.groupby(['zip', 'hour']).count()['Date/Time']).reset_index()\n",
    "    _demand_h.columns = ['Zip', 'Hour', 'Number of Trips']\n",
    "\n",
    "    _demand_wh = (_data.groupby(['zip', 'weekday', 'hour']).count()['Date/Time']).reset_index()\n",
    "    _demand_wh.columns = ['Zip', 'Weekday', 'Hour', 'Number of Trips']\n",
    "\n",
    "    return _data, _demand, _demand_w, _demand_h, _demand_wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a19c5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemandPredictorBase(object):\n",
    "    \"\"\"\n",
    "        Base class for demand predictor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, _x, _y, _prefix, train=True, output_dir=\".\"):\n",
    "        self.x = _x\n",
    "        self.y = _y\n",
    "        self.prefix = _prefix\n",
    "        self.output_dir = output_dir\n",
    "        self.model = self.build_model()\n",
    "        if train:\n",
    "            self.train()\n",
    "\n",
    "    def build_model(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, _x_test):\n",
    "        \"\"\"\n",
    "        :param _x_test: test dataset\n",
    "        :return: prediction for the given test dataset _x_test\n",
    "        \"\"\"\n",
    "        return self.model.predict(_x_test)\n",
    "\n",
    "    def predict_and_scale(self, _x_test, _y_scalar):\n",
    "        \"\"\"\n",
    "        :param _x_test: test dataset\n",
    "        :param _y_scalar: Scaler\n",
    "        :return: prediction for the given test dataset _x_test, scaled to the scalar\n",
    "        \"\"\"\n",
    "        return _y_scalar.inverse_transform(self.predict(_x_test))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mse(_y_test, _y_pred):\n",
    "        \"\"\"\n",
    "        :param _y_test: actual test values\n",
    "        :param _y_pred: predicted test values\n",
    "        :return: return the mean square error\n",
    "        \"\"\"\n",
    "        return mean_squared_error(_y_test, _y_pred)\n",
    "    \n",
    "    def save_model(self, _data_size, _model_id):\n",
    "        if self.model is not None:\n",
    "            import s3fs\n",
    "            from pickle import dump\n",
    "            fs = s3fs.S3FileSystem(anon=False)\n",
    "            bucket = f\"{self.output_dir}\"\n",
    "            file_name = f\"{self.prefix}_model_{_data_size}_{_model_id}.pickle\"\n",
    "            dump(self.model, fs.open(f\"s3://{bucket}/{file_name}\", 'wb'))\n",
    "\n",
    "\n",
    "class DemandPredictorNN(DemandPredictorBase):\n",
    "    def __init__(self, _x, _y, train=True, output_dir=\".\", hidden_layer_conf=None, d_key=\"\", dt_size=-1):\n",
    "        self.input_shape = len(_x[0])\n",
    "        self.output_shape = len(_y[0])\n",
    "        self.hidden_layer_conf = hidden_layer_conf\n",
    "        self.d_key = d_key\n",
    "        self.dt_size = dt_size\n",
    "        self.epochs = 100\n",
    "        self.batch_size = 150\n",
    "        self.verbose = 0\n",
    "        self.validation_split = 0.2\n",
    "        self.learning_rate = 0.01\n",
    "        self.history = None\n",
    "        super(DemandPredictorNN, self).__init__(_x, _y, \"nn\", train, output_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            build the model\n",
    "        \"\"\"\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=(self.input_shape,)))\n",
    "        if self.hidden_layer_conf is None:\n",
    "            model.add(Dense(168, activation='relu'))\n",
    "            model.add(Dropout(0.1))\n",
    "            model.add(Dense(24, activation='relu'))\n",
    "            model.add(Dropout(0.01))\n",
    "            model.add(Dense(self.output_shape, activation='linear'))\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        else:\n",
    "            for layer in self.hidden_layer_conf:\n",
    "                model.add(Dense(layer[\"neurons\"], activation=layer[\"activation\"]))\n",
    "        model.add(Dense(self.output_shape, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def predict(self, _x_test):\n",
    "        if os.path.exists(self.output_dir + f\"/best_model_{self.d_key}_{self.dt_size}.h5\"):\n",
    "            self.model = load_model(self.output_dir + f\"/best_model_{self.d_key}_{self.dt_size}.h5\")\n",
    "        return super(DemandPredictorNN, self).predict(_x_test)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            train the model\n",
    "        \"\"\"\n",
    "        self.history = \\\n",
    "            self.model.fit(\n",
    "                self.x, self.y,\n",
    "                epochs=self.epochs, batch_size=self.batch_size,\n",
    "                verbose=self.verbose, validation_split=self.validation_split,\n",
    "                use_multiprocessing=True\n",
    "            )\n",
    "\n",
    "    def save_model(self, _data_size, _model_id):\n",
    "        if self.model is not None:\n",
    "            import io\n",
    "            import s3fs\n",
    "            model_data = io.BytesIO()\n",
    "            self.model.save(model_data)\n",
    "            model_data.seek(0)\n",
    "            s3 = s3fs.S3FileSystem(anon=False)  # Uses default credentials\n",
    "            bucket = f\"{self.output_dir}\"\n",
    "            file_name = f\"{self.prefix}_model_{_data_size}_{_model_id}.h5\"\n",
    "            with s3.open(f's3://{bucket}/{file_name}', 'wb') as f:\n",
    "                f.write(model_data.getbuffer())  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38a0adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(_processed_data):\n",
    "    columns = list(_processed_data.columns)\n",
    "    columns.remove(\"Number of Trips\")\n",
    "    sc_x = StandardScaler()\n",
    "    sc_y = MinMaxScaler()\n",
    "    x = np.array(\n",
    "        [\n",
    "            [entry[col] for col in columns]\n",
    "            for _, entry in _processed_data.iterrows()\n",
    "        ]\n",
    "    )\n",
    "    y = np.transpose([_processed_data[\"Number of Trips\"].to_list()])\n",
    "    x = sc_x.fit_transform(x)\n",
    "    y = sc_y.fit_transform(y)\n",
    "    return x, y, sc_x, sc_y\n",
    "\n",
    "def generate_full_data(_processed_data, sc_x):\n",
    "    day_of_week = [w for w in range(7)]\n",
    "    hour_of_day = [h for h in range(24)]\n",
    "    zip_codes = list(_processed_data[\"Zip\"].unique())\n",
    "    from pandas import DataFrame\n",
    "    from itertools import product\n",
    "    df_full_sample = DataFrame(list(product(zip_codes, day_of_week, hour_of_day)),\n",
    "                                  columns=['Zip', 'Weekday', 'Hour'])\n",
    "    columns = list(df_full_sample.columns)\n",
    "    x = np.array(\n",
    "        [\n",
    "            [entry[col] for col in columns]\n",
    "            for _, entry in df_full_sample.iterrows()\n",
    "        ]\n",
    "    )\n",
    "    x = sc_x.fit_transform(x)\n",
    "    return df_full_sample, x\n",
    "\n",
    "def predict_using_neural_network(_processed_data, _demand_key, _output_dir, _hidden_layer_config, _data_size):\n",
    "    \"\"\"\n",
    "    :param _processed_data: processed_data\n",
    "    :param _demand_key: demand data type\n",
    "    :param _output_dir: output directory\n",
    "    :param _hidden_layer_config: hidden layer configuration\n",
    "    :param _data_size: data size\n",
    "    :return: run NUM_SAMPLES time neural network and compute the average MSE and MSE ratio\n",
    "    \"\"\"\n",
    "    _x, _y, _sc_x, _sc_y = transform_data(_processed_data)\n",
    "    nn = DemandPredictorNN(\n",
    "        _x=_x, _y=_y,\n",
    "        output_dir=_output_dir, hidden_layer_conf=_hidden_layer_config,\n",
    "        d_key=_demand_key, dt_size=_data_size\n",
    "    )\n",
    "    \n",
    "    _initial_df, _x_full_sample = generate_full_data(_processed_data, _sc_x)\n",
    "    _y_full_sample = nn.predict(_x_full_sample)\n",
    "    _y_full_sample = _sc_y.inverse_transform(_y_full_sample)\n",
    "    _y_full_sample_int = [int(_y / 26.1) if _y >= 0 else 0 for _y in _y_full_sample]\n",
    "    _y_full_sample_float = [float(_y / 26.1) if _y >= 0 else 0 for _y in _y_full_sample]\n",
    "    _initial_df[\"Expected Trips\"] = _y_full_sample_int\n",
    "    _initial_df[\"Expected Trips (Est)\"] = _y_full_sample_float\n",
    "    _initial_df.to_csv(f\"full_processed_sample_{_demand_key}_{_data_size}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa81a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function(_data_size, _input_dir, _output_dir):\n",
    "    \"\"\"\n",
    "    :param _data_size: size of the input data\n",
    "    :param _input_dir: input directory\n",
    "    :param _output_dir: output directory\n",
    "    \"\"\"\n",
    "    start = datetime.now()\n",
    "    data, _, _, _, demand_wh = load_data(\n",
    "        _input_dir=_input_dir, _data_size=_data_size, _output_dir=_output_dir\n",
    "    )\n",
    "    end = datetime.now()\n",
    "    print(f\"Data Loading Time : {(end - start).total_seconds()}\")\n",
    "    nn_hidden_layer_config = {\n",
    "        \"weekday_n_hour\": [\n",
    "            {\n",
    "                \"activation\": \"relu\", \"neurons\": 24\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    demand_data = {\n",
    "        \"weekday_n_hour\": demand_wh\n",
    "    }\n",
    "    for demand_key in demand_data:\n",
    "        demand_datum = demand_data[demand_key]\n",
    "        predict_using_neural_network(\n",
    "            demand_datum, demand_key, _output_dir=_output_dir,\n",
    "            _hidden_layer_config=nn_hidden_layer_config[demand_key], _data_size=_data_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f35dddb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading Time : 6.227063\n"
     ]
    }
   ],
   "source": [
    "data_size = 1000\n",
    "notebook_instance_id = \"xlarge\"\n",
    "input_dir = \"../../data\"\n",
    "output_dir = f\"../../output/output_{data_size}_{notebook_instance_id}\"\n",
    "main_function(data_size, input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775d50e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
