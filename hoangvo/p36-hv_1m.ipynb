{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b97145d0",
   "metadata": {},
   "source": [
    "conda amazonei tensorflow_p36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a70a09db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success - the MySageMakerInstance is in the us-east-1 region. You will use the 811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest container for your SageMaker endpoint.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "prefix = 'sagemaker/DEMO-xgboost-dm'\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", my_region, \"latest\")\n",
    "\n",
    "print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + xgboost_container + \" container for your SageMaker endpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e4994d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 error:  An error occurred (IllegalLocationConstraintException) when calling the CreateBucket operation: The unspecified location constraint is incompatible for the region specific endpoint this request was sent to.\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'awsgis' # <--- CHANGE THIS VARIABLE TO A UNIQUE NAME FOR YOUR BUCKET\n",
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "      s3.create_bucket(Bucket=bucket_name)\n",
    "    else: \n",
    "      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d5118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "NUM_SAMPLES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e45fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(_input_dir, _output_dir=\"output\", _data_size=-1):\n",
    "    \"\"\"\n",
    "    :param _input_dir: input directory name\n",
    "                      AWS S3 directory name, where the input files are stored\n",
    "    :param _output_dir: output directory name\n",
    "                      AWS S3 directory name, where the output files are saved\n",
    "    :param _data_size: size of data\n",
    "                      Data size, that needs to be tested, by default it takes value of\n",
    "                      -1, which means consider all the data\n",
    "    :return:\n",
    "            the processed data, and demand data\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from pandas import DataFrame\n",
    "\n",
    "    # load all the data\n",
    "    months = [\"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\"]\n",
    "    file_format = \"uber-processed-data-{}14.csv\"\n",
    "    _data = DataFrame()\n",
    "    for month in months:\n",
    "        file_name = _input_dir + \"/\" + file_format.format(month)\n",
    "        df_sub = pd.read_csv(file_name)\n",
    "        _data = _data.append(df_sub)\n",
    "\n",
    "    # sample the data\n",
    "    if _data_size > 0:\n",
    "        _data = _data.sample(n=_data_size)\n",
    "\n",
    "    # summarizing demand data\n",
    "    _demand = (_data.groupby(['zip']).count()['Date/Time']).reset_index()\n",
    "    _demand.columns = ['Zip', 'Number of Trips']\n",
    "    _demand.to_csv(_output_dir + \"/demand.csv\", index=False)\n",
    "\n",
    "    _demand_w = (_data.groupby(['zip', 'weekday']).count()['Date/Time']).reset_index()\n",
    "    _demand_w.columns = ['Zip', 'Weekday', 'Number of Trips']\n",
    "    _demand_w.to_csv(_output_dir + \"/demand_dow.csv\", index=False)\n",
    "\n",
    "    _demand_h = (_data.groupby(['zip', 'hour']).count()['Date/Time']).reset_index()\n",
    "    _demand_h.columns = ['Zip', 'Hour', 'Number of Trips']\n",
    "    _demand_h.to_csv(_output_dir + \"/demand_h.csv\", index=False)\n",
    "\n",
    "    _demand_wh = (_data.groupby(['zip', 'weekday', 'hour']).count()['Date/Time']).reset_index()\n",
    "    _demand_wh.columns = ['Zip', 'Weekday', 'Hour', 'Number of Trips']\n",
    "    _demand_wh.to_csv(_output_dir + \"/demand_h_dow.csv\", index=False)\n",
    "\n",
    "    return _data, _demand, _demand_w, _demand_h, _demand_wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3396133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemandPredictorBase(object):\n",
    "    \"\"\"\n",
    "        Base class for demand predictor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, _x, _y, _prefix, train=True, output_dir=\".\"):\n",
    "        self.x = _x\n",
    "        self.y = _y\n",
    "        self.prefix = _prefix\n",
    "        self.output_dir = output_dir\n",
    "        self.model = self.build_model()\n",
    "        if train:\n",
    "            self.train()\n",
    "\n",
    "    def build_model(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, _x_test):\n",
    "        \"\"\"\n",
    "        :param _x_test: test dataset\n",
    "        :return: prediction for the given test dataset _x_test\n",
    "        \"\"\"\n",
    "        return self.model.predict(_x_test)\n",
    "\n",
    "    def predict_and_scale(self, _x_test, _y_scalar):\n",
    "        \"\"\"\n",
    "        :param _x_test: test dataset\n",
    "        :param _y_scalar: Scaler\n",
    "        :return: prediction for the given test dataset _x_test, scaled to the scalar\n",
    "        \"\"\"\n",
    "        return _y_scalar.inverse_transform(self.predict(_x_test))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mse(_y_test, _y_pred):\n",
    "        \"\"\"\n",
    "        :param _y_test: actual test values\n",
    "        :param _y_pred: predicted test values\n",
    "        :return: return the mean square error\n",
    "        \"\"\"\n",
    "        return mean_squared_error(_y_test, _y_pred)\n",
    "    \n",
    "    def save_model(self, _data_size, _model_id):\n",
    "        if self.model is not None:\n",
    "            import s3fs\n",
    "            from pickle import dump\n",
    "            fs = s3fs.S3FileSystem(anon=False)\n",
    "            bucket = f\"{self.output_dir}\"\n",
    "            file_name = f\"{self.prefix}_model_{_data_size}_{_model_id}.pickle\"\n",
    "            dump(self.model, fs.open(f\"s3://{bucket}/{file_name}\", 'wb'))\n",
    "\n",
    "\n",
    "class DemandPredictorNN(DemandPredictorBase):\n",
    "    def __init__(self, _x, _y, train=True, output_dir=\".\", hidden_layer_conf=None, d_key=\"\", dt_size=-1):\n",
    "        self.input_shape = len(_x[0])\n",
    "        self.output_shape = len(_y[0])\n",
    "        self.hidden_layer_conf = hidden_layer_conf\n",
    "        self.d_key = d_key\n",
    "        self.dt_size = dt_size\n",
    "        self.epochs = 100\n",
    "        self.batch_size = 150\n",
    "        self.verbose = 0\n",
    "        self.validation_split = 0.2\n",
    "        self.learning_rate = 0.01\n",
    "        self.history = None\n",
    "        super(DemandPredictorNN, self).__init__(_x, _y, \"nn\", train, output_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            build the model\n",
    "        \"\"\"\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=(self.input_shape,)))\n",
    "        if self.hidden_layer_conf is None:\n",
    "            model.add(Dense(168, activation='relu'))\n",
    "            model.add(Dropout(0.1))\n",
    "            model.add(Dense(24, activation='relu'))\n",
    "            model.add(Dropout(0.01))\n",
    "            model.add(Dense(self.output_shape, activation='linear'))\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        else:\n",
    "            for layer in self.hidden_layer_conf:\n",
    "                model.add(Dense(layer[\"neurons\"], activation=layer[\"activation\"]))\n",
    "        model.add(Dense(self.output_shape, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def predict(self, _x_test):\n",
    "        if os.path.exists(self.output_dir + f\"/best_model_{self.d_key}_{self.dt_size}.h5\"):\n",
    "            self.model = load_model(self.output_dir + f\"/best_model_{self.d_key}_{self.dt_size}.h5\")\n",
    "        return super(DemandPredictorNN, self).predict(_x_test)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            train the model\n",
    "        \"\"\"\n",
    "        self.history = \\\n",
    "            self.model.fit(\n",
    "                self.x, self.y,\n",
    "                epochs=self.epochs, batch_size=self.batch_size,\n",
    "                verbose=self.verbose, validation_split=self.validation_split,\n",
    "                use_multiprocessing=True\n",
    "            )\n",
    "\n",
    "    def save_model(self, _data_size, _model_id):\n",
    "        if self.model is not None:\n",
    "            import io\n",
    "            import s3fs\n",
    "            model_data = io.BytesIO()\n",
    "            self.model.save(model_data)\n",
    "            model_data.seek(0)\n",
    "            s3 = s3fs.S3FileSystem(anon=False)  # Uses default credentials\n",
    "            bucket = f\"{self.output_dir}\"\n",
    "            file_name = f\"{self.prefix}_model_{_data_size}_{_model_id}.h5\"\n",
    "            with s3.open(f's3://{bucket}/{file_name}', 'wb') as f:\n",
    "                f.write(model_data.getbuffer())  \n",
    "\n",
    "    def plot(self, _i, _mse=None):\n",
    "        losses = self.history.history['loss']\n",
    "        val_losses = self.history.history['val_loss']\n",
    "        epochs = [i for i in range(len(losses))]\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel(\"Epochs [Log Scale]\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        legend = [\"Val Loss\", \"Train Loss\"]\n",
    "        if _mse is not None:\n",
    "            plt.plot(epochs, [_mse for _ in range(len(epochs))])\n",
    "            legend = [\"MSE\"] + legend\n",
    "        plt.plot(epochs, val_losses)\n",
    "        plt.plot(epochs, losses)\n",
    "        plt.legend(legend, loc=\"lower center\", bbox_to_anchor=(0.5, 0.0))\n",
    "        plt.title(\"Variation of Loss function over time\")\n",
    "        import io\n",
    "        import s3fs\n",
    "        img_data = io.BytesIO()\n",
    "        plt.savefig(img_data, format='pdf', bbox_inches='tight')\n",
    "        img_data.seek(0)\n",
    "        s3 = s3fs.S3FileSystem(anon=False)  # Uses default credentials\n",
    "        with s3.open(f'{_i}.pdf', 'wb') as f:\n",
    "            f.write(img_data.getbuffer())\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "class DemandPredictorSVR(DemandPredictorBase):\n",
    "    def __init__(self, _x, _y, train=True, output_dir=\".\"):\n",
    "        self.kernel = 'rbf'\n",
    "        self.gamma = 10\n",
    "        self.c = 10\n",
    "        super(DemandPredictorSVR, self).__init__(_x, _y, \"svr\", train, output_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            build the model\n",
    "        \"\"\"\n",
    "        model = SVR(kernel=self.kernel, gamma=self.gamma, C=self.c)\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            train the model\n",
    "        \"\"\"\n",
    "        self.model.fit(self.x, self.y)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "class DemandPredictorNB(DemandPredictorBase):\n",
    "    def __init__(self, _x, _y, train=True, output_dir=\".\"):\n",
    "\n",
    "        super(DemandPredictorNB, self).__init__(_x, _y, \"nb\", train, output_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            build the model\n",
    "        \"\"\"\n",
    "        model = GaussianNB()\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            train the model\n",
    "        \"\"\"\n",
    "        self.model.fit(self.x, self.y)\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "class DemandPredictorTree(DemandPredictorBase):\n",
    "    def __init__(self, _x, _y, train=True, output_dir=\".\"):\n",
    "        self.criterion = 'entropy'\n",
    "        self.max_depth = 10\n",
    "        self.splitter = 'best'\n",
    "        super(DemandPredictorTree, self).__init__(_x, _y, \"dt\", train, output_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            build the model\n",
    "        \"\"\"\n",
    "        model = tree.DecisionTreeClassifier(criterion = self.criterion, splitter = self.splitter, max_depth = self.max_depth)\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            train the model\n",
    "        \"\"\"\n",
    "        self.model.fit(self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a816b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(_processed_data):\n",
    "    columns = list(_processed_data.columns)\n",
    "    columns.remove(\"Number of Trips\")\n",
    "    sc_x = StandardScaler()\n",
    "    sc_y = MinMaxScaler()\n",
    "    x = np.array(\n",
    "        [\n",
    "            [entry[col] for col in columns]\n",
    "            for _, entry in _processed_data.iterrows()\n",
    "        ]\n",
    "    )\n",
    "    y = np.transpose([_processed_data[\"Number of Trips\"].to_list()])\n",
    "    x = sc_x.fit_transform(x)\n",
    "    y = sc_y.fit_transform(y)\n",
    "    return x, y, sc_x, sc_y\n",
    "\n",
    "def solve_using_neural_network(_processed_data, _demand_key, _output_dir, _hidden_layer_config, _data_size):\n",
    "    \"\"\"\n",
    "    :param _processed_data: processed_data\n",
    "    :param _demand_key: demand data type\n",
    "    :param _output_dir: output directory\n",
    "    :param _hidden_layer_config: hidden layer configuration\n",
    "    :param _data_size: data size\n",
    "    :return: run NUM_SAMPLES time neural network and compute the average MSE and MSE ratio\n",
    "    \"\"\"\n",
    "    _x, _y, _, _ = transform_data(_processed_data)\n",
    "\n",
    "    mse = []\n",
    "    time_taken = []\n",
    "\n",
    "    for _i in range(NUM_SAMPLES):\n",
    "        start = datetime.now()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.1, random_state=_i)\n",
    "        nn = DemandPredictorNN(\n",
    "            _x=x_train, _y=y_train,\n",
    "            output_dir=_output_dir, hidden_layer_conf=_hidden_layer_config,\n",
    "            d_key=_demand_key, dt_size=_data_size\n",
    "        )\n",
    "        y_pred = nn.predict(x_test)\n",
    "        mse_val = nn.get_mse(y_test, y_pred)\n",
    "        nn.plot(f\"{_output_dir}/demand_type_{_demand_key}_sample_id_{_i}\", mse_val)\n",
    "        mse.append(mse_val)\n",
    "        time_taken.append((datetime.now() - start).total_seconds())\n",
    "        nn.save_model(_data_size, _i)\n",
    "\n",
    "    print(f\"Average Time Taken: {np.mean(time_taken)} seconds\")\n",
    "    print(f\"Average MSE: {np.mean(mse)}, Min MSE: {min(mse)}, Max MSE: {max(mse)}\")\n",
    "\n",
    "def solve_using_svr(_processed_data, _demand_key, _output_dir, _data_size):\n",
    "    \"\"\"\n",
    "    :param _processed_data: processed_data\n",
    "    :param _demand_key: demand data type\n",
    "    :param _output_dir: output directory\n",
    "    :param _data_size: data size\n",
    "    :return: run NUM_SAMPLES time neural network and compute the average MSE and MSE ratio\n",
    "    \"\"\"\n",
    "    _x, _y, _, _ = transform_data(_processed_data)\n",
    "\n",
    "    mse = []\n",
    "    time_taken = []\n",
    "\n",
    "    for _i in range(NUM_SAMPLES):\n",
    "        start = datetime.now()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.1, random_state=_i)\n",
    "        svr = DemandPredictorSVR(x_train, y_train, output_dir=_output_dir)\n",
    "        y_pred = svr.predict(x_test)\n",
    "        mse.append(svr.get_mse(y_test, y_pred))\n",
    "        time_taken.append((datetime.now() - start).total_seconds())\n",
    "        svr.save_model(_data_size, _i)\n",
    "\n",
    "    print(f\"Average Time Taken: {np.mean(time_taken)} seconds\")\n",
    "    print(f\"Average MSE: {np.mean(mse)}, Min MSE: {min(mse)}, Max MSE: {max(mse)}\")\n",
    "\n",
    "def solve_using_nb(_processed_data, _demand_key, _output_dir, _data_size):\n",
    "    \"\"\"\n",
    "    :param _processed_data: processed_data\n",
    "    :param _demand_key: demand data type\n",
    "    :param _output_dir: output directory\n",
    "    :param _data_size: data size\n",
    "    :return: run NUM_SAMPLES time neural network and compute the average MSE and MSE ratio\n",
    "    \"\"\"\n",
    "    _x, _y, _, _ = transform_data(_processed_data)\n",
    "\n",
    "    mse = []\n",
    "    time_taken = []\n",
    "\n",
    "    for _i in range(NUM_SAMPLES):\n",
    "        start = datetime.now()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.1, random_state=_i)\n",
    "        nb = DemandPredictorNB(x_train, y_train, output_dir=_output_dir)\n",
    "        y_pred = nb.predict(x_test)\n",
    "        mse.append(nb.get_mse(y_test, y_pred))\n",
    "        time_taken.append((datetime.now() - start).total_seconds())\n",
    "        nb.save_model(_data_size, _i)\n",
    "\n",
    "    print(f\"Average Time Taken: {np.mean(time_taken)} seconds\")\n",
    "    print(f\"Average MSE: {np.mean(mse)}, Min MSE: {min(mse)}, Max MSE: {max(mse)}\")\n",
    "\n",
    "def solve_using_tree(_processed_data, _demand_key, _output_dir, _data_size):\n",
    "    \"\"\"\n",
    "    :param _processed_data: processed_data\n",
    "    :param _demand_key: demand data type\n",
    "    :param _output_dir: output directory\n",
    "    :param _data_size: data size\n",
    "    :return: run NUM_SAMPLES time neural network and compute the average MSE and MSE ratio\n",
    "    \"\"\"\n",
    "    _x, _y, _, _ = transform_data(_processed_data)\n",
    "\n",
    "    mse = []\n",
    "    time_taken = []\n",
    "\n",
    "    for _i in range(NUM_SAMPLES):\n",
    "        start = datetime.now()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.1, random_state=_i)\n",
    "        clf = DemandPredictorTree(x_train, y_train, output_dir=_output_dir)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        mse.append(clf.get_mse(y_test, y_pred))\n",
    "        time_taken.append((datetime.now() - start).total_seconds())\n",
    "        clf.save_model(_data_size, _i)\n",
    "\n",
    "    print(f\"Average Time Taken: {np.mean(time_taken)} seconds\")\n",
    "    print(f\"Average MSE: {np.mean(mse)}, Min MSE: {min(mse)}, Max MSE: {max(mse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e51044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function(_data_size, _input_dir, _output_dir):\n",
    "    \"\"\"\n",
    "    :param _data_size: size of the input data\n",
    "    :param _input_dir: input directory\n",
    "    :param _output_dir: output directory\n",
    "    \"\"\"\n",
    "    start = datetime.now()\n",
    "    data, _, _, _, demand_wh = load_data(\n",
    "        _input_dir=_input_dir, _data_size=_data_size, _output_dir=_output_dir\n",
    "    )\n",
    "    end = datetime.now()\n",
    "    print(f\"Data Loading Time : {(end - start).total_seconds()}\")\n",
    "    nn_hidden_layer_config = {\n",
    "        \"weekday_n_hour\": [\n",
    "            {\n",
    "                \"activation\": \"relu\", \"neurons\": 24\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    demand_data = {\n",
    "        \"weekday_n_hour\": demand_wh\n",
    "    }\n",
    "    for demand_key in demand_data:\n",
    "        demand_datum = demand_data[demand_key]\n",
    "        print(f\"checking {demand_key}\")\n",
    "        solve_using_svr(demand_datum, demand_key, _output_dir=_output_dir, _data_size=_data_size)\n",
    "        solve_using_neural_network(\n",
    "            demand_datum, demand_key, _output_dir=_output_dir,\n",
    "            _hidden_layer_config=nn_hidden_layer_config[demand_key], _data_size=_data_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47beae33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading Time : 15.547877\n",
      "checking weekday_n_hour\n",
      "Average Time Taken: 27.107619399999997 seconds\n",
      "Average MSE: 0.008903853966243307, Min MSE: 0.00865944199557561, Max MSE: 0.009119137812563703\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Average Time Taken: 18.753201400000002 seconds\n",
      "Average MSE: 0.0037647783879557585, Min MSE: 0.0035035015876708035, Max MSE: 0.004082335327624864\n"
     ]
    }
   ],
   "source": [
    "data_size = 1000000\n",
    "notebook_instance_id = \"xlarge\"\n",
    "input_dir = \"s3://cloud-project-x\"\n",
    "output_dir = f\"s3://cloud-project-x/output_{data_size}_{notebook_instance_id}\"\n",
    "main_function(data_size, input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c088a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61e7ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictnn = load_model('nn_model.h5')\n",
    "predictnn.load_weights('nn_weight.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a25aa9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading Time : 12.078445\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "output_dir = f\"s3://awsgis/output_test\"\n",
    "start = datetime.now()\n",
    "data, _, _, _, demand_wh = load_data(\n",
    "        _input_dir='s3://awsgis', _data_size=-1, _output_dir=output_dir\n",
    "    )\n",
    "end = datetime.now()\n",
    "print(f\"Data Loading Time : {(end - start).total_seconds()}\")\n",
    "\n",
    "_x, _y, _, _ = transform_data(demand_wh)\n",
    "y_pred = predictnn.predict(_x)\n",
    "#se_val = predictnn.get_mse(_y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a6a7002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05206466],\n",
       "       [-0.00183472],\n",
       "       [-0.00670257],\n",
       "       ...,\n",
       "       [ 0.00245023],\n",
       "       [ 0.00025514],\n",
       "       [ 0.00687551]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86b30084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.        ],\n",
       "       [0.00023469],\n",
       "       ...,\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85661685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zip</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Number of Trips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1566</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6410</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6450</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6460</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6461</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53789</th>\n",
       "      <td>11978</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53790</th>\n",
       "      <td>11980</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53791</th>\n",
       "      <td>11980</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53792</th>\n",
       "      <td>11980</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53793</th>\n",
       "      <td>11980</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53794 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Zip  Weekday  Hour  Number of Trips\n",
       "0       1566        6     8                1\n",
       "1       6410        4    14                1\n",
       "2       6450        4    10                2\n",
       "3       6460        5    11                1\n",
       "4       6461        6    19                1\n",
       "...      ...      ...   ...              ...\n",
       "53789  11978        5    20                1\n",
       "53790  11980        2    16                1\n",
       "53791  11980        2    18                1\n",
       "53792  11980        4    14                1\n",
       "53793  11980        6    21                1\n",
       "\n",
       "[53794 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demand_wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12afc86f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
