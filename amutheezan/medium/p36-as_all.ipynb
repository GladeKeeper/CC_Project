{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8c40882b",
   "metadata": {},
   "source": [
    "conda amazonei tensorflow_p36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d924f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success - the MySageMakerInstance is in the us-east-1 region. You will use the 811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest container for your SageMaker endpoint.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "prefix = 'sagemaker/DEMO-xgboost-dm'\n",
    "my_region = boto3.session.Session().region_name  # set the region of the instance\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", my_region, \"latest\")\n",
    "\n",
    "print(\n",
    "    \"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + xgboost_container + \" container for your SageMaker endpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077e6cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 error:  An error occurred (IllegalLocationConstraintException) when calling the CreateBucket operation: The unspecified location constraint is incompatible for the region specific endpoint this request was sent to.\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'awsgis'  # <--- CHANGE THIS VARIABLE TO A UNIQUE NAME FOR YOUR BUCKET\n",
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if my_region == 'us-east-1':\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    else:\n",
    "        s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': my_region})\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3714fdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uszipcode in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (0.2.6)\n",
      "Requirement already satisfied: pathlib-mate in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from uszipcode) (1.0.1)\n",
      "Requirement already satisfied: SQLAlchemy in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from uszipcode) (1.3.23)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from uszipcode) (21.2.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from uszipcode) (2.26.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pathlib-mate->uszipcode) (1.16.0)\n",
      "Requirement already satisfied: autopep8 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pathlib-mate->uszipcode) (1.5.5)\n",
      "Requirement already satisfied: atomicwrites in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pathlib-mate->uszipcode) (1.4.0)\n",
      "Requirement already satisfied: toml in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from autopep8->pathlib-mate->uszipcode) (0.10.2)\n",
      "Requirement already satisfied: pycodestyle>=2.6.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from autopep8->pathlib-mate->uszipcode) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->uszipcode) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->uszipcode) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->uszipcode) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->uszipcode) (1.26.7)\n",
      "\u001B[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow_p36/bin/python -m pip install --upgrade pip' command.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "NUM_SAMPLES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b12d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(_input_dir, _output_dir=\"output\", _data_size=-1):\n",
    "    \"\"\"\n",
    "    :param _input_dir: input directory name\n",
    "                      AWS S3 directory name, where the input files are stored\n",
    "    :param _output_dir: output directory name\n",
    "                      AWS S3 directory name, where the output files are saved\n",
    "    :param _data_size: size of data\n",
    "                      Data size, that needs to be tested, by default it takes value of\n",
    "                      -1, which means consider all the data\n",
    "    :return:\n",
    "            the processed data, and demand data\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from pandas import DataFrame\n",
    "\n",
    "    # load all the data\n",
    "    months = [\"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\"]\n",
    "    file_format = \"uber-processed-data-{}14.csv\"\n",
    "    _data = DataFrame()\n",
    "    for month in months:\n",
    "        file_name = _input_dir + \"/\" + file_format.format(month)\n",
    "        df_sub = pd.read_csv(file_name)\n",
    "        _data = _data.append(df_sub)\n",
    "\n",
    "    # sample the data\n",
    "    if _data_size > 0:\n",
    "        _data = _data.sample(n=_data_size)\n",
    "\n",
    "    # summarizing demand data\n",
    "    _demand = (_data.groupby(['zip']).count()['Date/Time']).reset_index()\n",
    "    _demand.columns = ['Zip', 'Number of Trips']\n",
    "    _demand.to_csv(_output_dir + \"/demand.csv\", index=False)\n",
    "\n",
    "    _demand_w = (_data.groupby(['zip', 'weekday']).count()['Date/Time']).reset_index()\n",
    "    _demand_w.columns = ['Zip', 'Weekday', 'Number of Trips']\n",
    "    _demand_w.to_csv(_output_dir + \"/demand_dow.csv\", index=False)\n",
    "\n",
    "    _demand_h = (_data.groupby(['zip', 'hour']).count()['Date/Time']).reset_index()\n",
    "    _demand_h.columns = ['Zip', 'Hour', 'Number of Trips']\n",
    "    _demand_h.to_csv(_output_dir + \"/demand_h.csv\", index=False)\n",
    "\n",
    "    _demand_wh = (_data.groupby(['zip', 'weekday', 'hour']).count()['Date/Time']).reset_index()\n",
    "    _demand_wh.columns = ['Zip', 'Weekday', 'Hour', 'Number of Trips']\n",
    "    _demand_wh.to_csv(_output_dir + \"/demand_h_dow.csv\", index=False)\n",
    "\n",
    "    return _data, _demand, _demand_w, _demand_h, _demand_wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b99f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemandPredictorBase(object):\n",
    "    \"\"\"\n",
    "        Base class for demand predictor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, _x, _y, _prefix, train=True, output_dir=\".\"):\n",
    "        self.x = _x\n",
    "        self.y = _y\n",
    "        self.prefix = _prefix\n",
    "        self.output_dir = output_dir\n",
    "        self.model = self.build_model()\n",
    "        if train:\n",
    "            self.train()\n",
    "\n",
    "    def build_model(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, _x_test):\n",
    "        \"\"\"\n",
    "        :param _x_test: test dataset\n",
    "        :return: prediction for the given test dataset _x_test\n",
    "        \"\"\"\n",
    "        return self.model.predict(_x_test)\n",
    "\n",
    "    def predict_and_scale(self, _x_test, _y_scalar):\n",
    "        \"\"\"\n",
    "        :param _x_test: test dataset\n",
    "        :param _y_scalar: Scaler\n",
    "        :return: prediction for the given test dataset _x_test, scaled to the scalar\n",
    "        \"\"\"\n",
    "        return _y_scalar.inverse_transform(self.predict(_x_test))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mse(_y_test, _y_pred):\n",
    "        \"\"\"\n",
    "        :param _y_test: actual test values\n",
    "        :param _y_pred: predicted test values\n",
    "        :return: return the mean square error\n",
    "        \"\"\"\n",
    "        return mean_squared_error(_y_test, _y_pred)\n",
    "\n",
    "    def save_model(self, _data_size, _model_id):\n",
    "        if self.model is not None:\n",
    "            import s3fs\n",
    "            from pickle import dump\n",
    "            fs = s3fs.S3FileSystem(anon=False)\n",
    "            bucket = f\"{self.output_dir}\"\n",
    "            file_name = f\"{self.prefix}_model_{_data_size}_{_model_id}.pickle\"\n",
    "            dump(self.model, fs.open(f\"s3://{bucket}/{file_name}\", 'wb'))\n",
    "\n",
    "\n",
    "class DemandPredictorNN(DemandPredictorBase):\n",
    "    def __init__(self, _x, _y, train=True, output_dir=\".\", hidden_layer_conf=None, d_key=\"\", dt_size=-1):\n",
    "        self.input_shape = len(_x[0])\n",
    "        self.output_shape = len(_y[0])\n",
    "        self.hidden_layer_conf = hidden_layer_conf\n",
    "        self.d_key = d_key\n",
    "        self.dt_size = dt_size\n",
    "        self.epochs = 100\n",
    "        self.batch_size = 150\n",
    "        self.verbose = 0\n",
    "        self.validation_split = 0.2\n",
    "        self.learning_rate = 0.01\n",
    "        self.history = None\n",
    "        super(DemandPredictorNN, self).__init__(_x, _y, \"nn\", train, output_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            build the model\n",
    "        \"\"\"\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=(self.input_shape,)))\n",
    "        if self.hidden_layer_conf is None:\n",
    "            model.add(Dense(168, activation='relu'))\n",
    "            model.add(Dropout(0.1))\n",
    "            model.add(Dense(24, activation='relu'))\n",
    "            model.add(Dropout(0.01))\n",
    "            model.add(Dense(self.output_shape, activation='linear'))\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        else:\n",
    "            for layer in self.hidden_layer_conf:\n",
    "                model.add(Dense(layer[\"neurons\"], activation=layer[\"activation\"]))\n",
    "        model.add(Dense(self.output_shape, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def predict(self, _x_test):\n",
    "        if os.path.exists(self.output_dir + f\"/best_model_{self.d_key}_{self.dt_size}.h5\"):\n",
    "            self.model = load_model(self.output_dir + f\"/best_model_{self.d_key}_{self.dt_size}.h5\")\n",
    "        return super(DemandPredictorNN, self).predict(_x_test)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            train the model\n",
    "        \"\"\"\n",
    "        self.history =\\\n",
    "            self.model.fit(\n",
    "                self.x, self.y,\n",
    "                epochs=self.epochs, batch_size=self.batch_size,\n",
    "                verbose=self.verbose, validation_split=self.validation_split,\n",
    "                use_multiprocessing=True\n",
    "            )\n",
    "\n",
    "    def save_model(self, _data_size, _model_id):\n",
    "        if self.model is not None:\n",
    "            import io\n",
    "            import s3fs\n",
    "            model_data = io.BytesIO()\n",
    "            self.model.save(model_data)\n",
    "            model_data.seek(0)\n",
    "            s3 = s3fs.S3FileSystem(anon=False)  # Uses default credentials\n",
    "            bucket = f\"{self.output_dir}\"\n",
    "            file_name = f\"{self.prefix}_model_{_data_size}_{_model_id}.h5\"\n",
    "            with s3.open(f's3://{bucket}/{file_name}', 'wb') as f:\n",
    "                f.write(model_data.getbuffer())\n",
    "\n",
    "    def plot(self, _i, _mse=None):\n",
    "        losses = self.history.history['loss']\n",
    "        val_losses = self.history.history['val_loss']\n",
    "        epochs = [i for i in range(len(losses))]\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel(\"Epochs [Log Scale]\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        legend = [\"Val Loss\", \"Train Loss\"]\n",
    "        if _mse is not None:\n",
    "            plt.plot(epochs, [_mse for _ in range(len(epochs))])\n",
    "            legend = [\"MSE\"] + legend\n",
    "        plt.plot(epochs, val_losses)\n",
    "        plt.plot(epochs, losses)\n",
    "        plt.legend(legend, loc=\"lower center\", bbox_to_anchor=(0.5, 0.0))\n",
    "        plt.title(\"Variation of Loss function over time\")\n",
    "        import io\n",
    "        import s3fs\n",
    "        img_data = io.BytesIO()\n",
    "        plt.savefig(img_data, format='pdf', bbox_inches='tight')\n",
    "        img_data.seek(0)\n",
    "        s3 = s3fs.S3FileSystem(anon=False)  # Uses default credentials\n",
    "        with s3.open(f'{_i}.pdf', 'wb') as f:\n",
    "            f.write(img_data.getbuffer())\n",
    "        plt.close()\n",
    "\n",
    "class DemandPredictorSVR(DemandPredictorBase):\n",
    "    def __init__(self, _x, _y, train=True, output_dir=\".\"):\n",
    "        self.kernel = 'rbf'\n",
    "        self.gamma = 10\n",
    "        self.c = 10\n",
    "        super(DemandPredictorSVR, self).__init__(_x, _y, \"svr\", train, output_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            build the model\n",
    "        \"\"\"\n",
    "        model = SVR(kernel=self.kernel, gamma=self.gamma, C=self.c)\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            train the model\n",
    "        \"\"\"\n",
    "        self.model.fit(self.x, self.y)\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "class DemandPredictorNB(DemandPredictorBase):\n",
    "    def __init__(self, _x, _y, train=True, output_dir=\".\"):\n",
    "        super(DemandPredictorNB, self).__init__(_x, _y, \"nb\", train, output_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            build the model\n",
    "        \"\"\"\n",
    "        model = GaussianNB()\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            train the model\n",
    "        \"\"\"\n",
    "        self.model.fit(self.x, self.y)\n",
    "\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "class DemandPredictorTree(DemandPredictorBase):\n",
    "    def __init__(self, _x, _y, train=True, output_dir=\".\"):\n",
    "        self.criterion = 'entropy'\n",
    "        self.max_depth = 10\n",
    "        self.splitter = 'best'\n",
    "        super(DemandPredictorTree, self).__init__(_x, _y, \"dt\", train, output_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            build the model\n",
    "        \"\"\"\n",
    "        model = tree.DecisionTreeClassifier(criterion=self.criterion, splitter=self.splitter, max_depth=self.max_depth)\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            train the model\n",
    "        \"\"\"\n",
    "        self.model.fit(self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ec905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(_processed_data):\n",
    "    columns = list(_processed_data.columns)\n",
    "    columns.remove(\"Number of Trips\")\n",
    "    sc_x = StandardScaler()\n",
    "    sc_y = MinMaxScaler()\n",
    "    x = np.array(\n",
    "        [\n",
    "            [entry[col] for col in columns]\n",
    "            for _, entry in _processed_data.iterrows()\n",
    "        ]\n",
    "    )\n",
    "    y = np.transpose([_processed_data[\"Number of Trips\"].to_list()])\n",
    "    x = sc_x.fit_transform(x)\n",
    "    y = sc_y.fit_transform(y)\n",
    "    return x, y, sc_x, sc_y\n",
    "\n",
    "\n",
    "def solve_using_neural_network(_processed_data, _demand_key, _output_dir, _hidden_layer_config, _data_size):\n",
    "    \"\"\"\n",
    "    :param _processed_data: processed_data\n",
    "    :param _demand_key: demand data type\n",
    "    :param _output_dir: output directory\n",
    "    :param _hidden_layer_config: hidden layer configuration\n",
    "    :param _data_size: data size\n",
    "    :return: run NUM_SAMPLES time neural network and compute the average MSE and MSE ratio\n",
    "    \"\"\"\n",
    "    _x, _y, _, _ = transform_data(_processed_data)\n",
    "\n",
    "    mse = []\n",
    "    time_taken = []\n",
    "\n",
    "    for _i in range(NUM_SAMPLES):\n",
    "        start = datetime.now()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.1, random_state=_i)\n",
    "        nn = DemandPredictorNN(\n",
    "            _x=x_train, _y=y_train,\n",
    "            output_dir=_output_dir, hidden_layer_conf=_hidden_layer_config,\n",
    "            d_key=_demand_key, dt_size=_data_size\n",
    "        )\n",
    "        y_pred = nn.predict(x_test)\n",
    "        mse_val = nn.get_mse(y_test, y_pred)\n",
    "        nn.plot(f\"{_output_dir}/demand_type_{_demand_key}_sample_id_{_i}\", mse_val)\n",
    "        mse.append(mse_val)\n",
    "        time_taken.append((datetime.now() - start).total_seconds())\n",
    "        nn.save_model(_data_size, _i)\n",
    "\n",
    "    print(f\"Average Time Taken: {np.mean(time_taken)} seconds\")\n",
    "    print(f\"Average MSE: {np.mean(mse)}, Min MSE: {min(mse)}, Max MSE: {max(mse)}\")\n",
    "\n",
    "\n",
    "def solve_using_svr(_processed_data, _demand_key, _output_dir, _data_size):\n",
    "    \"\"\"\n",
    "    :param _processed_data: processed_data\n",
    "    :param _demand_key: demand data type\n",
    "    :param _output_dir: output directory\n",
    "    :param _data_size: data size\n",
    "    :return: run NUM_SAMPLES time neural network and compute the average MSE and MSE ratio\n",
    "    \"\"\"\n",
    "    _x, _y, _, _ = transform_data(_processed_data)\n",
    "\n",
    "    mse = []\n",
    "    time_taken = []\n",
    "\n",
    "    for _i in range(NUM_SAMPLES):\n",
    "        start = datetime.now()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.1, random_state=_i)\n",
    "        svr = DemandPredictorSVR(x_train, y_train, output_dir=_output_dir)\n",
    "        y_pred = svr.predict(x_test)\n",
    "        mse.append(svr.get_mse(y_test, y_pred))\n",
    "        time_taken.append((datetime.now() - start).total_seconds())\n",
    "        svr.save_model(_data_size, _i)\n",
    "\n",
    "    print(f\"Average Time Taken: {np.mean(time_taken)} seconds\")\n",
    "    print(f\"Average MSE: {np.mean(mse)}, Min MSE: {min(mse)}, Max MSE: {max(mse)}\")\n",
    "\n",
    "\n",
    "def solve_using_nb(_processed_data, _demand_key, _output_dir, _data_size):\n",
    "    \"\"\"\n",
    "    :param _processed_data: processed_data\n",
    "    :param _demand_key: demand data type\n",
    "    :param _output_dir: output directory\n",
    "    :param _data_size: data size\n",
    "    :return: run NUM_SAMPLES time neural network and compute the average MSE and MSE ratio\n",
    "    \"\"\"\n",
    "    _x, _y, _, _ = transform_data(_processed_data)\n",
    "\n",
    "    mse = []\n",
    "    time_taken = []\n",
    "\n",
    "    for _i in range(NUM_SAMPLES):\n",
    "        start = datetime.now()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.1, random_state=_i)\n",
    "        nb = DemandPredictorNB(x_train, y_train, output_dir=_output_dir)\n",
    "        y_pred = nb.predict(x_test)\n",
    "        mse.append(nb.get_mse(y_test, y_pred))\n",
    "        time_taken.append((datetime.now() - start).total_seconds())\n",
    "        nb.save_model(_data_size, _i)\n",
    "\n",
    "    print(f\"Average Time Taken: {np.mean(time_taken)} seconds\")\n",
    "    print(f\"Average MSE: {np.mean(mse)}, Min MSE: {min(mse)}, Max MSE: {max(mse)}\")\n",
    "\n",
    "\n",
    "def solve_using_tree(_processed_data, _demand_key, _output_dir, _data_size):\n",
    "    \"\"\"\n",
    "    :param _processed_data: processed_data\n",
    "    :param _demand_key: demand data type\n",
    "    :param _output_dir: output directory\n",
    "    :param _data_size: data size\n",
    "    :return: run NUM_SAMPLES time neural network and compute the average MSE and MSE ratio\n",
    "    \"\"\"\n",
    "    _x, _y, _, _ = transform_data(_processed_data)\n",
    "\n",
    "    mse = []\n",
    "    time_taken = []\n",
    "\n",
    "    for _i in range(NUM_SAMPLES):\n",
    "        start = datetime.now()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.1, random_state=_i)\n",
    "        clf = DemandPredictorTree(x_train, y_train, output_dir=_output_dir)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        mse.append(clf.get_mse(y_test, y_pred))\n",
    "        time_taken.append((datetime.now() - start).total_seconds())\n",
    "        clf.save_model(_data_size, _i)\n",
    "\n",
    "    print(f\"Average Time Taken: {np.mean(time_taken)} seconds\")\n",
    "    print(f\"Average MSE: {np.mean(mse)}, Min MSE: {min(mse)}, Max MSE: {max(mse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4924a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function(_data_size, _input_dir, _output_dir):\n",
    "    \"\"\"\n",
    "    :param _data_size: size of the input data\n",
    "    :param _input_dir: input directory\n",
    "    :param _output_dir: output directory\n",
    "    \"\"\"\n",
    "    start = datetime.now()\n",
    "    data, _, _, _, demand_wh = load_data(\n",
    "        _input_dir=_input_dir, _data_size=_data_size, _output_dir=_output_dir\n",
    "    )\n",
    "    end = datetime.now()\n",
    "    print(f\"Data Loading Time : {(end - start).total_seconds()}\")\n",
    "    nn_hidden_layer_config = {\n",
    "        \"weekday_n_hour\": [\n",
    "            {\n",
    "                \"activation\": \"relu\", \"neurons\": 24\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    demand_data = {\n",
    "        \"weekday_n_hour\": demand_wh\n",
    "    }\n",
    "    for demand_key in demand_data:\n",
    "        demand_datum = demand_data[demand_key]\n",
    "        print(f\"checking {demand_key}\")\n",
    "        solve_using_svr(demand_datum, demand_key, _output_dir=_output_dir, _data_size=_data_size)\n",
    "        #         solve_using_nb(demand_datum, demand_key, _output_dir=_output_dir, _data_size=_data_size)\n",
    "        #         solve_using_tree(demand_datum, demand_key, _output_dir=_output_dir, _data_size=_data_size)\n",
    "        solve_using_neural_network(\n",
    "            demand_datum, demand_key, _output_dir=_output_dir,\n",
    "            _hidden_layer_config=nn_hidden_layer_config[demand_key], _data_size=_data_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = -1\n",
    "notebook_instance_id = \"medium\"\n",
    "input_dir = \"s3://cloud-project-x\"\n",
    "output_dir = f\"s3://cloud-project-x/output_{data_size}_{notebook_instance_id}\"\n",
    "main_function(data_size, input_dir, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}