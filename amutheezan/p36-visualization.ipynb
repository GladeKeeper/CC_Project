{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4fee67b2",
   "metadata": {},
   "source": [
    "conda amazonei tensorflow_p36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a920a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success - the MySageMakerInstance is in the us-east-1 region. You will use the 811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest container for your SageMaker endpoint.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "prefix = 'sagemaker/DEMO-xgboost-dm'\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", my_region, \"latest\")\n",
    "\n",
    "print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + xgboost_container + \" container for your SageMaker endpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f565393d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 error:  An error occurred (IllegalLocationConstraintException) when calling the CreateBucket operation: The unspecified location constraint is incompatible for the region specific endpoint this request was sent to.\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'awsgis' # <--- CHANGE THIS VARIABLE TO A UNIQUE NAME FOR YOUR BUCKET\n",
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "      s3.create_bucket(Bucket=bucket_name)\n",
    "    else: \n",
    "      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc46fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!pip install folium\n",
    "import folium\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51906faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(_input_dir, _data_size=-1):\n",
    "    \"\"\"\n",
    "    :param _input_dir: input directory name\n",
    "                      AWS S3 directory name, where the input files are stored\n",
    "    :param _data_size: size of data\n",
    "                      Data size, that needs to be tested, by default it takes value of\n",
    "                      -1, which means consider all the data\n",
    "    :return:\n",
    "            the processed data, and demand data\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from pandas import DataFrame\n",
    "\n",
    "    # load all the data\n",
    "    months = [\"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\"]\n",
    "    file_format = \"uber-processed-data-{}14.csv\"\n",
    "    _data = DataFrame()\n",
    "    for month in months:\n",
    "        file_name = _input_dir + \"/\" + file_format.format(month)\n",
    "        df_sub = pd.read_csv(file_name)\n",
    "        _data = _data.append(df_sub)\n",
    "\n",
    "    # sample the data\n",
    "    if _data_size > 0:\n",
    "        _data = _data.sample(n=_data_size)\n",
    "\n",
    "    _demand_wh = (_data.groupby(['zip', 'weekday', 'hour']).count()['Date/Time']).reset_index()\n",
    "    _demand_wh.columns = ['Zip', 'Weekday', 'Hour', 'Number of Trips']\n",
    "\n",
    "    return _demand_wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "85b01a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_wh = load_data(\n",
    "    _input_dir=\"s3://cloud-project-x\", _data_size=-1,\n",
    ")\n",
    "\n",
    "import io\n",
    "import s3fs\n",
    "import json\n",
    "import boto3 \n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = 'cloud-project-x'\n",
    "key = 'nyc.geojson'\n",
    "obj = s3.Object(bucket, key)\n",
    "data = obj.get()['Body'].read().decode('utf-8')\n",
    "tmp = json.loads(data)\n",
    "\n",
    "geozips = []\n",
    "for i in range(len(tmp['features'])):\n",
    "    if int(tmp['features'][i]['properties']['postalCode']) in list(demand_wh['Zip'].unique()):\n",
    "        tmp[\"features\"][i]['properties']['postalCode'] = int(tmp[\"features\"][i]['properties']['postalCode'])\n",
    "        geozips.append(tmp['features'][i])\n",
    "\n",
    "new_json = dict.fromkeys(['type','features'])\n",
    "new_json['type'] = 'FeatureCollection'\n",
    "new_json['features'] = geozips\n",
    "\n",
    "s3object = s3.Object('cloud-project-x', 'updated-file.json')\n",
    "s3object.put(\n",
    "    Body=(bytes(json.dumps(new_json, sort_keys=True, indent=4, separators=(',', ': ')).encode('UTF-8')))\n",
    ")\n",
    "\n",
    "def create_map(table, zips, mapped_feature):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = 'cloud-project-x'\n",
    "    key = 'updated-file.json'\n",
    "    obj = s3.Object(bucket, key)\n",
    "    data = obj.get()['Body'].read().decode('utf-8')\n",
    "    ny_geo = json.loads(data)\n",
    "\n",
    "    m = folium.Map(location = [40.7128, -74.0060], zoom_start = 11)\n",
    "    m.choropleth(\n",
    "        geo_data = ny_geo,\n",
    "        fill_opacity = 1,\n",
    "        line_opacity = 0.2,\n",
    "        data = table,\n",
    "        key_on = 'feature.properties.postalCode',\n",
    "        columns = [zips, mapped_feature],\n",
    "        fill_color = 'YlGnBu',\n",
    "        legend_name = (' ').join(mapped_feature.split('_')).title() + ' Across NY'\n",
    "    )\n",
    "    folium.LayerControl().add_to(m)\n",
    "    m.save(outfile = mapped_feature + '_map.html')\n",
    "    with open(mapped_feature + '_map.html', 'rb') as f:\n",
    "        s3 = boto3.client('s3')\n",
    "        s3.put_object(Bucket='cloud-project-x', Key=mapped_feature + '_map.html', Body=f)\n",
    "\n",
    "create_map(demand_wh, 'Zip', 'Number of Trips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ca122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
