{
 "cells": [
  {
   "cell_type": "raw",
   "id": "87acd61a",
   "metadata": {},
   "source": [
    "conda amazonei tensorflow_p36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3a750ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success - the MySageMakerInstance is in the us-east-1 region. You will use the 811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest container for your SageMaker endpoint.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "prefix = 'sagemaker/DEMO-xgboost-dm'\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", my_region, \"latest\")\n",
    "\n",
    "print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + xgboost_container + \" container for your SageMaker endpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a22c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 error:  An error occurred (IllegalLocationConstraintException) when calling the CreateBucket operation: The unspecified location constraint is incompatible for the region specific endpoint this request was sent to.\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'awsgis' # <--- CHANGE THIS VARIABLE TO A UNIQUE NAME FOR YOUR BUCKET\n",
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "      s3.create_bucket(Bucket=bucket_name)\n",
    "    else: \n",
    "      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b31ac03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "NUM_SAMPLES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4adb0d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(_input_dir, _output_dir=\"output\", _data_size=-1):\n",
    "    \"\"\"\n",
    "    :param _input_dir: input directory name\n",
    "                      AWS S3 directory name, where the input files are stored\n",
    "    :param _output_dir: output directory name\n",
    "                      AWS S3 directory name, where the output files are saved\n",
    "    :param _data_size: size of data\n",
    "                      Data size, that needs to be tested, by default it takes value of\n",
    "                      -1, which means consider all the data\n",
    "    :return:\n",
    "            the processed data, and demand data\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from pandas import DataFrame\n",
    "\n",
    "    # load all the data\n",
    "    months = [\"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\"]\n",
    "    file_format = \"uber-processed-data-{}14.csv\"\n",
    "    _data = DataFrame()\n",
    "    for month in months:\n",
    "        file_name = _input_dir + \"/\" + file_format.format(month)\n",
    "        df_sub = pd.read_csv(file_name)\n",
    "        _data = _data.append(df_sub)\n",
    "\n",
    "    # sample the data\n",
    "    if _data_size > 0:\n",
    "        _data = _data.sample(n=_data_size)\n",
    "\n",
    "    # summarizing demand data\n",
    "    _demand = (_data.groupby(['zip']).count()['Date/Time']).reset_index()\n",
    "    _demand.columns = ['Zip', 'Number of Trips']\n",
    "    _demand.to_csv(_output_dir + \"/demand.csv\", index=False)\n",
    "\n",
    "    _demand_w = (_data.groupby(['zip', 'weekday']).count()['Date/Time']).reset_index()\n",
    "    _demand_w.columns = ['Zip', 'Weekday', 'Number of Trips']\n",
    "    _demand_w.to_csv(_output_dir + \"/demand_dow.csv\", index=False)\n",
    "\n",
    "    _demand_h = (_data.groupby(['zip', 'hour']).count()['Date/Time']).reset_index()\n",
    "    _demand_h.columns = ['Zip', 'Hour', 'Number of Trips']\n",
    "    _demand_h.to_csv(_output_dir + \"/demand_h.csv\", index=False)\n",
    "\n",
    "    _demand_wh = (_data.groupby(['zip', 'weekday', 'hour']).count()['Date/Time']).reset_index()\n",
    "    _demand_wh.columns = ['Zip', 'Weekday', 'Hour', 'Number of Trips']\n",
    "    _demand_wh.to_csv(_output_dir + \"/demand_h_dow.csv\", index=False)\n",
    "\n",
    "    return _data, _demand, _demand_w, _demand_h, _demand_wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a19c5259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemandPredictorBase(object):\n",
    "    \"\"\"\n",
    "        Base class for demand predictor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, _x, _y, _prefix, train=True, output_dir=\".\"):\n",
    "        self.x = _x\n",
    "        self.y = _y\n",
    "        self.prefix = _prefix\n",
    "        self.output_dir = output_dir\n",
    "        self.model = self.build_model()\n",
    "        if train:\n",
    "            self.train()\n",
    "\n",
    "    def build_model(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, _x_test):\n",
    "        \"\"\"\n",
    "        :param _x_test: test dataset\n",
    "        :return: prediction for the given test dataset _x_test\n",
    "        \"\"\"\n",
    "        return self.model.predict(_x_test)\n",
    "\n",
    "    def predict_and_scale(self, _x_test, _y_scalar):\n",
    "        \"\"\"\n",
    "        :param _x_test: test dataset\n",
    "        :param _y_scalar: Scaler\n",
    "        :return: prediction for the given test dataset _x_test, scaled to the scalar\n",
    "        \"\"\"\n",
    "        return _y_scalar.inverse_transform(self.predict(_x_test))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mse(_y_test, _y_pred):\n",
    "        \"\"\"\n",
    "        :param _y_test: actual test values\n",
    "        :param _y_pred: predicted test values\n",
    "        :return: return the mean square error\n",
    "        \"\"\"\n",
    "        return mean_squared_error(_y_test, _y_pred)\n",
    "    \n",
    "    def save_model(self, _data_size, _model_id):\n",
    "        if self.model is not None:\n",
    "            import s3fs\n",
    "            from pickle import dump\n",
    "            fs = s3fs.S3FileSystem(anon=False)\n",
    "            bucket = f\"{self.output_dir}\"\n",
    "            file_name = f\"{self.prefix}_model_{_data_size}_{_model_id}.pickle\"\n",
    "            dump(self.model, fs.open(f\"s3://{bucket}/{file_name}\", 'wb'))\n",
    "\n",
    "\n",
    "class DemandPredictorNN(DemandPredictorBase):\n",
    "    def __init__(self, _x, _y, train=True, output_dir=\".\", hidden_layer_conf=None, d_key=\"\", dt_size=-1):\n",
    "        self.input_shape = len(_x[0])\n",
    "        self.output_shape = len(_y[0])\n",
    "        self.hidden_layer_conf = hidden_layer_conf\n",
    "        self.d_key = d_key\n",
    "        self.dt_size = dt_size\n",
    "        self.epochs = 100\n",
    "        self.batch_size = 150\n",
    "        self.verbose = 0\n",
    "        self.validation_split = 0.2\n",
    "        self.learning_rate = 0.01\n",
    "        self.history = None\n",
    "        super(DemandPredictorNN, self).__init__(_x, _y, \"nn\", train, output_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            build the model\n",
    "        \"\"\"\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=(self.input_shape,)))\n",
    "        if self.hidden_layer_conf is None:\n",
    "            model.add(Dense(168, activation='relu'))\n",
    "            model.add(Dropout(0.1))\n",
    "            model.add(Dense(24, activation='relu'))\n",
    "            model.add(Dropout(0.01))\n",
    "            model.add(Dense(self.output_shape, activation='linear'))\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        else:\n",
    "            for layer in self.hidden_layer_conf:\n",
    "                model.add(Dense(layer[\"neurons\"], activation=layer[\"activation\"]))\n",
    "        model.add(Dense(self.output_shape, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def predict(self, _x_test):\n",
    "        if os.path.exists(self.output_dir + f\"/best_model_{self.d_key}_{self.dt_size}.h5\"):\n",
    "            self.model = load_model(self.output_dir + f\"/best_model_{self.d_key}_{self.dt_size}.h5\")\n",
    "        return super(DemandPredictorNN, self).predict(_x_test)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            train the model\n",
    "        \"\"\"\n",
    "        self.history = \\\n",
    "            self.model.fit(\n",
    "                self.x, self.y,\n",
    "                epochs=self.epochs, batch_size=self.batch_size,\n",
    "                verbose=self.verbose, validation_split=self.validation_split,\n",
    "                use_multiprocessing=True\n",
    "            )\n",
    "\n",
    "    def save_model(self, _data_size, _model_id):\n",
    "        if self.model is not None:\n",
    "            import io\n",
    "            import s3fs\n",
    "            model_data = io.BytesIO()\n",
    "            self.model.save(model_data)\n",
    "            model_data.seek(0)\n",
    "            s3 = s3fs.S3FileSystem(anon=False)  # Uses default credentials\n",
    "            bucket = f\"{self.output_dir}\"\n",
    "            file_name = f\"{self.prefix}_model_{_data_size}_{_model_id}.h5\"\n",
    "            with s3.open(f's3://{bucket}/{file_name}', 'wb') as f:\n",
    "                f.write(model_data.getbuffer())  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a0adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(_processed_data):\n",
    "    columns = list(_processed_data.columns)\n",
    "    columns.remove(\"Number of Trips\")\n",
    "    sc_x = StandardScaler()\n",
    "    sc_y = MinMaxScaler()\n",
    "    x = np.array(\n",
    "        [\n",
    "            [entry[col] for col in columns]\n",
    "            for _, entry in _processed_data.iterrows()\n",
    "        ]\n",
    "    )\n",
    "    y = np.transpose([_processed_data[\"Number of Trips\"].to_list()])\n",
    "    x = sc_x.fit_transform(x)\n",
    "    y = sc_y.fit_transform(y)\n",
    "    return x, y, sc_x, sc_y\n",
    "\n",
    "def generate_full_data(_processed_data, sc_x):\n",
    "    day_of_week = [w for w in range(7)]\n",
    "    hour_of_day = [h for h in range(24)]\n",
    "    zip_codes = _processed_data[\"Zip\"].unique().to_list()\n",
    "    from pandas import DataFrame\n",
    "    from itertools import product\n",
    "    df_full_sample = pd.DataFrame(list(product(zip_codes, day_of_week, hour_of_day)),\n",
    "                                  columns=['Zip', 'Weekday', 'Hour'])\n",
    "    df_full_sample = list(_processed_data.columns)\n",
    "    x = np.array(\n",
    "        [\n",
    "            [entry[col] for col in columns]\n",
    "            for _, entry in df_full_sample.iterrows()\n",
    "        ]\n",
    "    )\n",
    "    x = sc_x.fit_transform(x)\n",
    "    return df_full_sample, x\n",
    "\n",
    "def predict_using_neural_network(_processed_data, _demand_key, _output_dir, _hidden_layer_config, _data_size):\n",
    "    \"\"\"\n",
    "    :param _processed_data: processed_data\n",
    "    :param _demand_key: demand data type\n",
    "    :param _output_dir: output directory\n",
    "    :param _hidden_layer_config: hidden layer configuration\n",
    "    :param _data_size: data size\n",
    "    :return: run NUM_SAMPLES time neural network and compute the average MSE and MSE ratio\n",
    "    \"\"\"\n",
    "    _x, _y, _sc_x, _sc_y = transform_data(_processed_data)\n",
    "    nn = DemandPredictorNN(\n",
    "        _x=_x, _y=_y,\n",
    "        output_dir=_output_dir, hidden_layer_conf=_hidden_layer_config,\n",
    "        d_key=_demand_key, dt_size=_data_size\n",
    "    )\n",
    "    \n",
    "    _initial_df, _x_full_sample = generate_full_data(_processed_data, _sc_x)\n",
    "    _y_full_sample = nn.predict(_x_full_sample)\n",
    "    _y_full_sample = _sc_y.inverse_transform(_y_full_sample)\n",
    "    _initial_df[\"Number of Trips\"] = _y_full_sample\n",
    "    _initial_df.to_csv(f\"full_processed_sample_{_demand_key}_{_data_size}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa81a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function(_data_size, _input_dir, _output_dir):\n",
    "    \"\"\"\n",
    "    :param _data_size: size of the input data\n",
    "    :param _input_dir: input directory\n",
    "    :param _output_dir: output directory\n",
    "    \"\"\"\n",
    "    start = datetime.now()\n",
    "    data, _, _, _, demand_wh = load_data(\n",
    "        _input_dir=_input_dir, _data_size=_data_size, _output_dir=_output_dir\n",
    "    )\n",
    "    end = datetime.now()\n",
    "    print(f\"Data Loading Time : {(end - start).total_seconds()}\")\n",
    "    nn_hidden_layer_config = {\n",
    "        \"weekday_n_hour\": [\n",
    "            {\n",
    "                \"activation\": \"relu\", \"neurons\": 24\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    demand_data = {\n",
    "        \"weekday_n_hour\": demand_wh\n",
    "    }\n",
    "    for demand_key in demand_data:\n",
    "        demand_datum = demand_data[demand_key]\n",
    "        predict_using_neural_network(\n",
    "            demand_datum, demand_key, _output_dir=_output_dir,\n",
    "            _hidden_layer_config=nn_hidden_layer_config[demand_key], _data_size=_data_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f35dddb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output/output_1000_xlarge/demand.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/_m/bgd5p56n2xjf2tj9vdmd1wjw0000gn/T/ipykernel_37043/1688326645.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0minput_dir\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"../../data\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0moutput_dir\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf\"output/output_{data_size}_{notebook_instance_id}\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mmain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput_dir\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/var/folders/_m/bgd5p56n2xjf2tj9vdmd1wjw0000gn/T/ipykernel_37043/1106967295.py\u001B[0m in \u001B[0;36mmain_function\u001B[0;34m(_data_size, _input_dir, _output_dir)\u001B[0m\n\u001B[1;32m      6\u001B[0m     \"\"\"\n\u001B[1;32m      7\u001B[0m     \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdatetime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m     data, _, _, _, demand_wh = load_data(\n\u001B[0m\u001B[1;32m      9\u001B[0m         \u001B[0m_input_dir\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0m_input_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_data_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0m_data_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_output_dir\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0m_output_dir\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m     )\n",
      "\u001B[0;32m/var/folders/_m/bgd5p56n2xjf2tj9vdmd1wjw0000gn/T/ipykernel_37043/3105823745.py\u001B[0m in \u001B[0;36mload_data\u001B[0;34m(_input_dir, _output_dir, _data_size)\u001B[0m\n\u001B[1;32m     30\u001B[0m     \u001B[0m_demand\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0m_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'zip'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'Date/Time'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m     \u001B[0m_demand\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'Zip'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'Number of Trips'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 32\u001B[0;31m     \u001B[0m_demand\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_output_dir\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\"/demand.csv\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     33\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m     \u001B[0m_demand_w\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0m_data\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'zip'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'weekday'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'Date/Time'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/Paratransit8/lib/python3.8/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36mto_csv\u001B[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[0m\n\u001B[1;32m   3385\u001B[0m         )\n\u001B[1;32m   3386\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3387\u001B[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001B[0m\u001B[1;32m   3388\u001B[0m             \u001B[0mpath_or_buf\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3389\u001B[0m             \u001B[0mline_terminator\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mline_terminator\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/Paratransit8/lib/python3.8/site-packages/pandas/io/formats/format.py\u001B[0m in \u001B[0;36mto_csv\u001B[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[0m\n\u001B[1;32m   1081\u001B[0m             \u001B[0mformatter\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfmt\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1082\u001B[0m         )\n\u001B[0;32m-> 1083\u001B[0;31m         \u001B[0mcsv_formatter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1084\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1085\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcreated_buffer\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/Paratransit8/lib/python3.8/site-packages/pandas/io/formats/csvs.py\u001B[0m in \u001B[0;36msave\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    226\u001B[0m         \"\"\"\n\u001B[1;32m    227\u001B[0m         \u001B[0;31m# apply compression and byte/text conversion\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 228\u001B[0;31m         with get_handle(\n\u001B[0m\u001B[1;32m    229\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    230\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.conda/envs/Paratransit8/lib/python3.8/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    640\u001B[0m                 \u001B[0merrors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"replace\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    641\u001B[0m             \u001B[0;31m# Encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 642\u001B[0;31m             handle = open(\n\u001B[0m\u001B[1;32m    643\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    644\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'output/output_1000_xlarge/demand.csv'"
     ]
    }
   ],
   "source": [
    "data_size = 1000\n",
    "notebook_instance_id = \"xlarge\"\n",
    "input_dir = \"../../data\"\n",
    "output_dir = f\"output/output_{data_size}_{notebook_instance_id}\"\n",
    "main_function(data_size, input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775d50e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}