{
 "cells": [
  {
   "cell_type": "raw",
   "id": "404edb7f",
   "metadata": {},
   "source": [
    "conda amazonei tensorflow_p36\n",
    "\n",
    "conda amazonei tensorflow2 p36: do not have the suitable scipy package for the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1cc404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import boto3, re, sys, math, json, os, sagemaker, urllib.request\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "prefix = 'sagemaker/DEMO-xgboost-dm'\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", my_region, \"latest\")\n",
    "\n",
    "print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + xgboost_container + \" container for your SageMaker endpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c1bd28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae154623",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boto3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fb329ce2ccc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbucket_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'awsgis'\u001b[0m \u001b[0;31m# <--- CHANGE THIS VARIABLE TO A UNIQUE NAME FOR YOUR BUCKET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ms3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m  \u001b[0mmy_region\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'us-east-1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_bucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'boto3' is not defined"
     ]
    }
   ],
   "source": [
    "bucket_name = 'awsgis' # <--- CHANGE THIS VARIABLE TO A UNIQUE NAME FOR YOUR BUCKET\n",
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "      s3.create_bucket(Bucket=bucket_name)\n",
    "    else: \n",
    "      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed716dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uszipcode in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (0.2.6)\n",
      "Requirement already satisfied: pathlib-mate in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from uszipcode) (1.0.1)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from uszipcode) (21.2.0)\n",
      "Requirement already satisfied: SQLAlchemy in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from uszipcode) (1.3.23)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from uszipcode) (2.26.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pathlib-mate->uszipcode) (1.16.0)\n",
      "Requirement already satisfied: atomicwrites in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pathlib-mate->uszipcode) (1.4.0)\n",
      "Requirement already satisfied: autopep8 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from pathlib-mate->uszipcode) (1.5.5)\n",
      "Requirement already satisfied: toml in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from autopep8->pathlib-mate->uszipcode) (0.10.2)\n",
      "Requirement already satisfied: pycodestyle>=2.6.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from autopep8->pathlib-mate->uszipcode) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->uszipcode) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->uszipcode) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->uszipcode) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from requests->uszipcode) (1.26.7)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/cpu/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install uszipcode\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout\n",
    "from tensorflow.keras.models import model_from_json, load_model\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "NUM_SAMPLES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40194a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(_input_dir, _output_dir=\"output\", _data_size=-1):\n",
    "    \"\"\"\n",
    "    :param _input_dir: input directory name\n",
    "                      AWS S3 directory name, where the input files are stored\n",
    "    :param _output_dir: output directory name\n",
    "                      AWS S3 directory name, where the output files are saved\n",
    "    :param _data_size: size of data\n",
    "                      Data size, that needs to be tested, by default it takes value of\n",
    "                      -1, which means consider all the data\n",
    "    :return:\n",
    "            the processed data, and demand data\n",
    "    \"\"\"\n",
    "    import os.path\n",
    "    import pandas as pd\n",
    "    from pandas import DataFrame\n",
    "    from uszipcode import SearchEngine\n",
    "\n",
    "    # to obtain the zip-codes for latitude, longitude values\n",
    "    engine = SearchEngine()\n",
    "\n",
    "    # load all the data\n",
    "    months = [\"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\"]\n",
    "    file_format = \"uber-raw-data-{}14.csv\"\n",
    "    _data = DataFrame()\n",
    "    for month in months:\n",
    "        file_name = _input_dir + \"/\" + file_format.format(month)\n",
    "        df_sub = pd.read_csv(file_name)\n",
    "        _data = _data.append(df_sub)\n",
    "\n",
    "    # sample the data\n",
    "    if _data_size > 0:\n",
    "        _data = _data.sample(n=_data_size)\n",
    "\n",
    "    # process date and time\n",
    "    _data['Date/Time'] = pd.to_datetime(_data['Date/Time'], format='%m/%d/%Y %H:%M:%S')\n",
    "    _data['month'] = _data['Date/Time'].dt.month\n",
    "    _data['weekday'] = _data['Date/Time'].dt.dayofweek\n",
    "    _data['day'] = _data['Date/Time'].dt.day\n",
    "    _data['hour'] = _data['Date/Time'].dt.hour\n",
    "    _data['minute'] = _data['Date/Time'].dt.minute\n",
    "    _data['lat_short'] = round(_data['Lat'], 2)\n",
    "    _data['lon_short'] = round(_data['Lon'], 2)\n",
    "\n",
    "    # obtaining the zip-codes\n",
    "    _data['zip'] = _data.apply(\n",
    "        lambda row: engine.by_coordinates(row['Lat'], row['Lon'], radius=10)[0].zipcode, axis=1\n",
    "    )\n",
    "\n",
    "    # summarizing demand data\n",
    "    _demand = (_data.groupby(['zip']).count()['Date/Time']).reset_index()\n",
    "    _demand.columns = ['Zip', 'Number of Trips']\n",
    "    _demand.to_csv(_output_dir + \"/demand.csv\", index=False)\n",
    "\n",
    "    _demand_w = (_data.groupby(['zip', 'weekday']).count()['Date/Time']).reset_index()\n",
    "    _demand_w.columns = ['Zip', 'Weekday', 'Number of Trips']\n",
    "    _demand_w.to_csv(_output_dir + \"/demand_dow.csv\", index=False)\n",
    "\n",
    "    _demand_h = (_data.groupby(['zip', 'hour']).count()['Date/Time']).reset_index()\n",
    "    _demand_h.columns = ['Zip', 'Hour', 'Number of Trips']\n",
    "    _demand_h.to_csv(_output_dir + \"/demand_h.csv\", index=False)\n",
    "\n",
    "    _demand_wh = (_data.groupby(['zip', 'weekday', 'hour']).count()['Date/Time']).reset_index()\n",
    "    _demand_wh.columns = ['Zip', 'Weekday', 'Hour', 'Number of Trips']\n",
    "    _demand_wh.to_csv(_output_dir + \"/demand_h_dow.csv\", index=False)\n",
    "\n",
    "    return _data, _demand, _demand_w, _demand_h, _demand_wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c649fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemandPredictorBase(object):\n",
    "    \"\"\"\n",
    "        Base class for demand predictor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, _x, _y, _prefix, train=True, output_dir=\".\"):\n",
    "        self.x = _x\n",
    "        self.y = _y\n",
    "        self.prefix = _prefix\n",
    "        self.output_dir = output_dir\n",
    "        self.model = self.build_model()\n",
    "        if train:\n",
    "            self.train()\n",
    "\n",
    "    def build_model(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, _x_test):\n",
    "        \"\"\"\n",
    "        :param _x_test: test dataset\n",
    "        :return: prediction for the given test dataset _x_test\n",
    "        \"\"\"\n",
    "        return self.model.predict(_x_test)\n",
    "\n",
    "    def predict_and_scale(self, _x_test, _y_scalar):\n",
    "        \"\"\"\n",
    "        :param _x_test: test dataset\n",
    "        :param _y_scalar: Scaler\n",
    "        :return: prediction for the given test dataset _x_test, scaled to the scalar\n",
    "        \"\"\"\n",
    "        return _y_scalar.inverse_transform(self.predict(_x_test))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mse(_y_test, _y_pred):\n",
    "        \"\"\"\n",
    "        :param _y_test: actual test values\n",
    "        :param _y_pred: predicted test values\n",
    "        :return: return the mean square error\n",
    "        \"\"\"\n",
    "        return mean_squared_error(_y_test, _y_pred)\n",
    "    \n",
    "    def save_model(self, _data_size, _model_id):\n",
    "        if self.model is not None:\n",
    "            import s3fs\n",
    "            from pickle import dump\n",
    "            fs = s3fs.S3FileSystem(anon=False)\n",
    "            bucket = f\"{self.output_dir}\"\n",
    "            file_name = f\"{self.prefix}_model_{_data_size}_{_model_id}.pickle\"\n",
    "            dump(self.model, fs.open(f\"s3://{bucket}/{file_name}\", 'wb'))\n",
    "\n",
    "\n",
    "class DemandPredictorNN(DemandPredictorBase):\n",
    "    def __init__(self, _x, _y, train=True, output_dir=\".\", hidden_layer_conf=None, d_key=\"\", dt_size=-1):\n",
    "        self.input_shape = len(_x[0])\n",
    "        self.output_shape = len(_y[0])\n",
    "        self.hidden_layer_conf = hidden_layer_conf\n",
    "        self.d_key = d_key\n",
    "        self.dt_size = dt_size\n",
    "        self.epochs = 4000\n",
    "        self.batch_size = 150\n",
    "        self.verbose = 0\n",
    "        self.validation_split = 0.2\n",
    "        self.learning_rate = 0.01\n",
    "        self.history = None\n",
    "        super(DemandPredictorNN, self).__init__(_x, _y, \"nn\", train, output_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            build the model\n",
    "        \"\"\"\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(InputLayer(input_shape=(self.input_shape,)))\n",
    "        if self.hidden_layer_conf is None:\n",
    "            model.add(Dense(168, activation='relu'))\n",
    "            model.add(Dropout(0.1))\n",
    "            model.add(Dense(24, activation='relu'))\n",
    "            model.add(Dropout(0.01))\n",
    "            model.add(Dense(self.output_shape, activation='linear'))\n",
    "            model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        else:\n",
    "            for layer in self.hidden_layer_conf:\n",
    "                model.add(Dense(layer[\"neurons\"], activation=layer[\"activation\"]))\n",
    "        model.add(Dense(self.output_shape, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def predict(self, _x_test):\n",
    "        if os.path.exists(self.output_dir + f\"/best_model_{self.d_key}_{self.dt_size}.h5\"):\n",
    "            self.model = load_model(self.output_dir + f\"/best_model_{self.d_key}_{self.dt_size}.h5\")\n",
    "        return super(DemandPredictorNN, self).predict(_x_test)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            train the model\n",
    "        \"\"\"\n",
    "#         es = EarlyStopping(monitor='val_loss', mode='min', verbose=self.verbose, patience=100)\n",
    "#         mc = ModelCheckpoint(\n",
    "#             self.output_dir + f'/best_model_{self.d_key}_{self.dt_size}.h5', monitor='val_loss', mode='min',\n",
    "#             verbose=self.verbose, save_best_only=True\n",
    "#         )\n",
    "#         self.history = \\\n",
    "#             self.model.fit(\n",
    "#                 self.x, self.y,\n",
    "#                 epochs=self.epochs, batch_size=self.batch_size,\n",
    "#                 verbose=self.verbose, validation_split=self.validation_split,\n",
    "#                 use_multiprocessing=True, callbacks=[es, mc]\n",
    "#             )\n",
    "        self.history = \\\n",
    "            self.model.fit(\n",
    "                self.x, self.y,\n",
    "                epochs=self.epochs, batch_size=self.batch_size,\n",
    "                verbose=self.verbose, validation_split=self.validation_split,\n",
    "                use_multiprocessing=True\n",
    "            )\n",
    "\n",
    "    def save_model(self, _data_size, _model_id):\n",
    "        if self.model is not None:\n",
    "            import io\n",
    "            import s3fs\n",
    "            model_data = io.BytesIO()\n",
    "            self.model.save(model_data)\n",
    "            model_data.seek(0)\n",
    "            s3 = s3fs.S3FileSystem(anon=False)  # Uses default credentials\n",
    "            bucket = f\"{self.output_dir}\"\n",
    "            file_name = f\"{self.prefix}_model_{_data_size}_{_model_id}.h5\"\n",
    "            with s3.open(f's3://{bucket}/{file_name}', 'wb') as f:\n",
    "                f.write(model_data.getbuffer())  \n",
    "\n",
    "    def plot(self, _i, _mse=None):\n",
    "        losses = self.history.history['loss']\n",
    "        val_losses = self.history.history['val_loss']\n",
    "        epochs = [i for i in range(len(losses))]\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel(\"Epochs [Log Scale]\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        legend = [\"Val Loss\", \"Train Loss\"]\n",
    "        if _mse is not None:\n",
    "            plt.plot(epochs, [_mse for _ in range(len(epochs))])\n",
    "            legend = [\"MSE\"] + legend\n",
    "        plt.plot(epochs, val_losses)\n",
    "        plt.plot(epochs, losses)\n",
    "        plt.legend(legend, loc=\"lower center\", bbox_to_anchor=(0.5, 0.0))\n",
    "        plt.title(\"Variation of Loss function over time\")\n",
    "        plt.show()\n",
    "        import io\n",
    "        import s3fs\n",
    "        img_data = io.BytesIO()\n",
    "        plt.savefig(img_data, format='pdf', bbox_inches='tight')\n",
    "        img_data.seek(0)\n",
    "        s3 = s3fs.S3FileSystem(anon=False)  # Uses default credentials\n",
    "        with s3.open(f'{_i}.pdf', 'wb') as f:\n",
    "            f.write(img_data.getbuffer())\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "class DemandPredictorSVR(DemandPredictorBase):\n",
    "    def __init__(self, _x, _y, train=True, output_dir=\".\"):\n",
    "        self.kernel = 'rbf'\n",
    "        self.gamma = 10\n",
    "        self.c = 10\n",
    "        super(DemandPredictorSVR, self).__init__(_x, _y, \"svr\", train, output_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            build the model\n",
    "        \"\"\"\n",
    "        model = SVR(kernel=self.kernel, gamma=self.gamma, C=self.c)\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            train the model\n",
    "        \"\"\"\n",
    "        self.model.fit(self.x, self.y)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "class DemandPredictorNB(DemandPredictorBase):\n",
    "    def __init__(self, _x, _y, train=True, output_dir=\".\"):\n",
    "\n",
    "        super(DemandPredictorNB, self).__init__(_x, _y, \"nb\", train, output_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            build the model\n",
    "        \"\"\"\n",
    "        model = GaussianNB()\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            train the model\n",
    "        \"\"\"\n",
    "        self.model.fit(self.x, self.y)\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "class DemandPredictorTree(DemandPredictorBase):\n",
    "    def __init__(self, _x, _y, train=True, output_dir=\".\"):\n",
    "        self.criterion = 'entropy'\n",
    "        #self.max_depth = 10\n",
    "        self.splitter = 'best'\n",
    "        super(DemandPredictorTree, self).__init__(_x, _y, \"dt\", train, output_dir)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            build the model\n",
    "        \"\"\"\n",
    "        #model = tree.DecisionTreeClassifier(criterion = self.criterion, splitter = self.splitter )#, max_depth = self.max_depth)\n",
    "        model = tree.DecisionTreeRegressor(splitter = self.splitter)\n",
    "        return model\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "            train the model\n",
    "        \"\"\"\n",
    "  #      self.y = self.y.astype('int')\n",
    "   #     self.x = self.x.astype('float')\n",
    "        self.model.fit(self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ae09ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(_processed_data):\n",
    "    columns = list(_processed_data.columns)\n",
    "    columns.remove(\"Number of Trips\")\n",
    "    sc_x = StandardScaler()\n",
    "    sc_y = MinMaxScaler()\n",
    "    x = np.array(\n",
    "        [\n",
    "            [entry[col] for col in columns]\n",
    "            for _, entry in _processed_data.iterrows()\n",
    "        ]\n",
    "    )\n",
    "    y = np.transpose([_processed_data[\"Number of Trips\"].to_list()])\n",
    "    x = sc_x.fit_transform(x)\n",
    "    y = sc_y.fit_transform(y)\n",
    "    return x, y, sc_x, sc_y\n",
    "\n",
    "def solve_using_neural_network(_processed_data, _demand_key, _output_dir, _hidden_layer_config, _data_size):\n",
    "    \"\"\"\n",
    "    :param _processed_data: processed_data\n",
    "    :param _demand_key: demand data type\n",
    "    :param _output_dir: output directory\n",
    "    :param _hidden_layer_config: hidden layer configuration\n",
    "    :param _data_size: data size\n",
    "    :return: run NUM_SAMPLES time neural network and compute the average MSE and MSE ratio\n",
    "    \"\"\"\n",
    "    _x, _y, _, _ = transform_data(_processed_data)\n",
    "\n",
    "    mse = []\n",
    "    time_taken = []\n",
    "\n",
    "    for _i in range(NUM_SAMPLES):\n",
    "        start = datetime.now()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.1, random_state=_i)\n",
    "        nn = DemandPredictorNN(\n",
    "            _x=x_train, _y=y_train,\n",
    "            output_dir=_output_dir, hidden_layer_conf=_hidden_layer_config,\n",
    "            d_key=_demand_key, dt_size=_data_size\n",
    "        )\n",
    "        y_pred = nn.predict(x_test)\n",
    "        mse_val = nn.get_mse(y_test, y_pred)\n",
    "        nn.plot(f\"{_output_dir}/demand_type_{_demand_key}_sample_id_{_i}\", mse_val)\n",
    "        mse.append(mse_val)\n",
    "        time_taken.append((datetime.now() - start).total_seconds())\n",
    "        nn.save_model(_data_size, _i)\n",
    "\n",
    "    print(f\"Average Time Taken: {np.mean(time_taken)} seconds\")\n",
    "    print(f\"Average MSE: {np.mean(mse)}, Min MSE: {min(mse)}, Max MSE: {max(mse)}\")\n",
    "\n",
    "def solve_using_svr(_processed_data, _demand_key, _output_dir, _data_size):\n",
    "    \"\"\"\n",
    "    :param _processed_data: processed_data\n",
    "    :param _demand_key: demand data type\n",
    "    :param _output_dir: output directory\n",
    "    :param _data_size: data size\n",
    "    :return: run NUM_SAMPLES time neural network and compute the average MSE and MSE ratio\n",
    "    \"\"\"\n",
    "    _x, _y, _, _ = transform_data(_processed_data)\n",
    "\n",
    "    mse = []\n",
    "    time_taken = []\n",
    "\n",
    "    for _i in range(NUM_SAMPLES):\n",
    "        start = datetime.now()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.1, random_state=_i)\n",
    "        svr = DemandPredictorSVR(x_train, y_train, output_dir=_output_dir)\n",
    "        y_pred = svr.predict(x_test)\n",
    "        mse.append(svr.get_mse(y_test, y_pred))\n",
    "        time_taken.append((datetime.now() - start).total_seconds())\n",
    "        svr.save_model(_data_size, _i)\n",
    "\n",
    "    print(f\"Average Time Taken: {np.mean(time_taken)} seconds\")\n",
    "    print(f\"Average MSE: {np.mean(mse)}, Min MSE: {min(mse)}, Max MSE: {max(mse)}\")\n",
    "\n",
    "def solve_using_nb(_processed_data, _demand_key, _output_dir, _data_size):\n",
    "    \"\"\"\n",
    "    :param _processed_data: processed_data\n",
    "    :param _demand_key: demand data type\n",
    "    :param _output_dir: output directory\n",
    "    :param _data_size: data size\n",
    "    :return: run NUM_SAMPLES time neural network and compute the average MSE and MSE ratio\n",
    "    \"\"\"\n",
    "    _x, _y, _, _ = transform_data(_processed_data)\n",
    "\n",
    "    mse = []\n",
    "    time_taken = []\n",
    "\n",
    "    for _i in range(NUM_SAMPLES):\n",
    "        start = datetime.now()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.1, random_state=_i)\n",
    "        nb = DemandPredictorNB(x_train, y_train, output_dir=_output_dir)\n",
    "        y_pred = nb.predict(x_test)\n",
    "        mse.append(nb.get_mse(y_test, y_pred))\n",
    "        time_taken.append((datetime.now() - start).total_seconds())\n",
    "        nb.save_model(_data_size, _i)\n",
    "\n",
    "    print(f\"Average Time Taken: {np.mean(time_taken)} seconds\")\n",
    "    print(f\"Average MSE: {np.mean(mse)}, Min MSE: {min(mse)}, Max MSE: {max(mse)}\")\n",
    "\n",
    "def solve_using_tree(_processed_data, _demand_key, _output_dir, _data_size):\n",
    "    \"\"\"\n",
    "    :param _processed_data: processed_data\n",
    "    :param _demand_key: demand data type\n",
    "    :param _output_dir: output directory\n",
    "    :param _data_size: data size\n",
    "    :return: run NUM_SAMPLES time neural network and compute the average MSE and MSE ratio\n",
    "    \"\"\"\n",
    "    _x, _y, _, _ = transform_data(_processed_data)\n",
    "\n",
    "    mse = []\n",
    "    time_taken = []\n",
    "\n",
    "    for _i in range(NUM_SAMPLES):\n",
    "        start = datetime.now()\n",
    "        x_train, x_test, y_train, y_test = train_test_split(_x, _y, test_size=0.1, random_state=_i)\n",
    "        clf = DemandPredictorTree(x_train, y_train, output_dir=_output_dir)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        mse.append(clf.get_mse(y_test, y_pred))\n",
    "        time_taken.append((datetime.now() - start).total_seconds())\n",
    "        clf.save_model(_data_size, _i)\n",
    "\n",
    "    print(f\"Average Time Taken: {np.mean(time_taken)} seconds\")\n",
    "    print(f\"Average MSE: {np.mean(mse)}, Min MSE: {min(mse)}, Max MSE: {max(mse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd138576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function(_data_size, _input_dir, _output_dir):\n",
    "    \"\"\"\n",
    "    :param _data_size: size of the input data\n",
    "    :param _input_dir: input directory\n",
    "    :param _output_dir: output directory\n",
    "    \"\"\"\n",
    "    data, _, _, _, demand_wh = load_data(\n",
    "        _input_dir=_input_dir, _data_size=_data_size, _output_dir=_output_dir\n",
    "    )\n",
    "    nn_hidden_layer_config = {\n",
    "        \"weekday_n_hour\": [\n",
    "            {\n",
    "                \"activation\": \"relu\", \"neurons\": 168\n",
    "            },\n",
    "            {\n",
    "                \"activation\": \"relu\", \"neurons\": 24\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    demand_data = {\n",
    "        \"weekday_n_hour\": demand_wh\n",
    "    }\n",
    "    for demand_key in demand_data:\n",
    "        demand_datum = demand_data[demand_key]\n",
    "        print(f\"checking {demand_key}\")\n",
    "#        solve_using_svr(demand_datum, demand_key, _output_dir=_output_dir, _data_size=_data_size)\n",
    "        solve_using_nb(demand_datum, demand_key, _output_dir=_output_dir, _data_size=_data_size)\n",
    "#         solve_using_tree(demand_datum, demand_key, _output_dir=_output_dir, _data_size=_data_size)\n",
    "        solve_using_neural_network(\n",
    "            demand_datum, demand_key, _output_dir=_output_dir,\n",
    "            _hidden_layer_config=nn_hidden_layer_config[demand_key], _data_size=_data_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2da9363d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking weekday_n_hour\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: (array([0.        , 0.07692308, 0.15384615, 0.23076923, 0.30769231,\n       0.38461538, 0.46153846, 0.53846154, 0.61538462, 0.69230769,\n       0.76923077, 0.84615385, 1.        ]),)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4b8127260a31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"s3://nosqlgis\"\u001b[0m\u001b[0;31m#\"s3://cloud-project-x\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"s3://awsgis/output_{data_size}_{notebook_instance_id}_tree\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-175188667842>\u001b[0m in \u001b[0;36mmain_function\u001b[0;34m(_data_size, _input_dir, _output_dir)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"checking {demand_key}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#        solve_using_svr(demand_datum, demand_key, _output_dir=_output_dir, _data_size=_data_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0msolve_using_nb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdemand_datum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdemand_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_output_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_output_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_data_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_data_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;31m#         solve_using_tree(demand_datum, demand_key, _output_dir=_output_dir, _data_size=_data_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         solve_using_neural_network(\n",
      "\u001b[0;32m<ipython-input-20-3a73aaff0be7>\u001b[0m in \u001b[0;36msolve_using_nb\u001b[0;34m(_processed_data, _demand_key, _output_dir, _data_size)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDemandPredictorNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_output_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mmse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-72394de128eb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, _x, _y, train, output_dir)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDemandPredictorNB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-72394de128eb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, _x, _y, _prefix, train, output_dir)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-72394de128eb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mtrain\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \"\"\"\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[0;32m--> 210\u001b[0;31m                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, classes, _refit, sample_weight)\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_check_partial_fit_first_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m             \u001b[0;31m# This is the first call to partial_fit:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;31m# initialize various cumulative counters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36m_check_partial_fit_first_call\u001b[0;34m(clf, classes)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;31m# This is the first call to partial_fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0m_unique_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FN_UNIQUE_LABELS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_unique_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mys_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: (array([0.        , 0.07692308, 0.15384615, 0.23076923, 0.30769231,\n       0.38461538, 0.46153846, 0.53846154, 0.61538462, 0.69230769,\n       0.76923077, 0.84615385, 1.        ]),)"
     ]
    }
   ],
   "source": [
    "data_size = 10000 \n",
    "notebook_instance_id = 10 \n",
    "input_dir = \"s3://nosqlgis\"#\"s3://cloud-project-x\"\n",
    "output_dir = f\"s3://awsgis/output_{data_size}_{notebook_instance_id}_tree\"\n",
    "main_function(data_size, input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d1dfb76",
   "metadata": {},
   "source": [
    "conda tensorflow p36: slow processing, but work"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94ad233b",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/40215510/valueerror-unknown-label-type-array-while-using-decision-tree-classifier-and-u/40215746\n",
    "https://stackoverflow.com/questions/45346550/valueerror-unknown-label-type-unknown"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
